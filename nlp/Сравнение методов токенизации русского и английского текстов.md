Хорошо. Я подготовлю сравнение всех основных подходов к токенизации текста для русского и английского языков с фокусом на:

* Качество понимания смысла (semantic alignment)
* Следование инструкциям (instruction following)
* Качество сегментации (granularity, устойчивость к шуму)
* Соответствие морфологии (учёт грамматических границ, лемм и форм)

Я также оценю, какие типы токенизации применяются в современных моделях (например, LLM) и какие дают наилучшие метрики на практике.


# Сравнение методов токенизации русского и английского текстов

**Классические методы (whitespace, rule-based, NLTK, Moses)**. Простейшая токенизация по пробелам просто разбивает текст на слова по пробелам, игнорируя морфологию и часто прицепляя к словам пунктуацию или эмодзи, что даёт «сырая» сегментацию. Например, в такой токенизации «дом.» и «дом» — разные токены и смысловая связь между ними теряется. Правила (regex) типа Moses-разбиения или NLTK (англ.) учитывают пунктуацию, скобки, кавычки и пр.: они отделяют знаки препинания от слов и обрабатывают некоторые сокращения (“don’t” → “do” + “n’t”). Такие правила хорошо отлажены для английского (с учётом апострофов, чисел и т.д.), но они обычно англоязычны и могут искажать другие языки. Например, классический препроцессор GPT-2 разделяет английские сокращения («I’m» → “I” + “‘m”), что упрочняет структуру для English, но аналогичные правила могут разорвать слова в других языках на неестественных местах. Moses-токенизатор в системах перевода также широко используется для английского и русск­ого, но и он не учитывает морфемы (лишь отделяет пунктуацию, кавычки и т.д.). В целом классические подходы быстродействуют и понятны, но плохо справляются с морфологией, шумом и новыми символами (например, эмодзи обычно либо остаются прикреплёнными к слову, либо становятся отдельным «токеном», что делает сегментацию неустойчивой к шуму).

**BPE и WordPiece (субсловные модели)**. Алгоритм BPE (Byte-Pair Encoding) и близкий к нему WordPiece строят словарь из подслов на основании частот или вероятностной выгоды. Они широко используются в современных NLP-моделях (например, многие системы перевода и GPT-2). Для английского (конкатенативная морфология) BPE хорошо работает: он захватывает распространённые морфемы (например, “ing”, “tion”), уменьшая число OOV. Для русского (словоизменительный язык) стандартный BPE часто разрезает слова через морфемные границы без учёта их смысла, создавая неоднозначные токены. Как отмечают исследования, в морфологически богатых языках BPE-токенизация **«часто игнорирует значимые границы морфем, вводит неоднозначность и нарушает смысл»**. WordPiece отличается от BPE лишь критерием слияния (максимизирует прирост вероятности данных), но по сути даёт схожие разбиения. Оба метода по сути «машинно» находят частые буквосочетания без лингвистического учёта: это дает хороший компромисс «универсальности», но не гарантирует семантического соответствия токенов морфемам.

* **Семантическая согласованность**: BPE/WordPiece «срезают» части слов без учёта морфологии, поэтому в смысле лингвистики они плохо совпадают с единицами значения. К примеру, знакомые корни могут оказаться разбитыми или смешанными с суффиксами. Это может повышать нагрузку на модель, т.к. смысловые фрагменты не выделяются чётко.
* **Следование инструкциям**: BPE/WordPiece часто применяются «как есть» в английских LLM, поэтому для английских инструкций они подходят. Но для русского такие токенизаторы создают много коротких токенов (каждая форма слова – отдельный набор подслов), что затрудняет понимание текста моделью. Исследования по адаптации LLaMA к русскому показывают, что обычный BPE-токенизатор LLaMA дал худшее качество генерации, чем модификации (напр., Unigram).
* **Сегментация и устойчивость**: BPE обычно предтокенизует по пробелам, затем строит подслова. Пунктуация отделяется (в зависимости от конкретной настройки), но в целом BPE стабилен к стандартным символам. Он плохо справляется с эмодзи: если эмодзи встречаются впервые, они разбиваются по байтам и дают много новых токенов. Кроме того, классический BPE не учитывает шум (ошибки в словах будут рассмотрены как последовательность символов без общей статистики).
* **Морфология**: BPE никак не учитывает грамматические границы. Он просто объединяет наиболее частые последовательности символов. В английском (линейные афиксы) это не критично, а вот в русском всё словообразование оказало­ся сложнее: BPE может взять обрывок корня, приклеить окончание без анализа, и токен потеряет связь с леммой. Исследования отмечают, что Unigram и морфологически ориентированные токенизаторы лучше сохраняют семантическую целостность слов.

**SentencePiece и Unigram LM**. Модуль SentencePiece (Google) может строить словарь как через BPE, так и через Unigram (алгоритм Kudo). Он включает пробелы в алфавит, что позволяет токенам начинаться с «▁» (символ пробела), сохраняя информацию о границах слов. Различия: если SentencePiece+BPE – это по сути тот же BPE (с учётом пробелов), то UnigramLM – топ–даун подход. Unigram стартует с большого словаря фрагментов и рекурсивно отсекает наименее вероятные, оценивая потери лог-вероятности. Такой метод чаще сохраняет корни и суффиксы целиком: как показано, Unigram-токенизация лучше «сохраняет корневые/стемовые» части слова, что особенно полезно для русского. Эмпирически, модели, адаптированные с Unigram-токенизацией, добивались лучшего качества понимания смысла и большей согласованности морфологии. Например, при адаптации LLaMA к русскому Unigram-кодировка заметно повысила качество ответов модели и субъективное предпочтение людей.

* **Семантическая согласованность**: Уникальная особенность Unigram – он склонен выдавать более «семантические» токены (корни слов) благодаря оптимизации вероятности на глобальном уровне. Модель, токенизированная Unigram’ом, достигает лучших результатов по пониманию языка (наборы задач Russian SuperGLUE) по сравнению с BPE. Английский пример: MorphPiece – морфологическая версия Unigram – при обучении GPT-2 показала заметно более низкую перплексии и лучшую точность на задаче LAMBADA (предсказание закрытого слова). Так, MorphPiece-GPT давал \~58.6% точности против 46.9% у исходного GPT-2 (прирост \~+25%) на LAMBADA.
* **Следование инструкциям**: В исследованиях отмечено, что модели с Unigram-токенизацией дают более релевантные ответы на русские инструкции, увеличивая качество за счёт лучшего выравнивания токенов с морфологической структурой. Это означает, что модель эффективнее использует информацию в запросах.
* **Сегментация и устойчивость**: Unigram аналогично BPE предтокенизует по пробелам, но набор токенов может быть богаче. Он может лучше «собирать» редкие последовательности, но не гарантирует обработку случайного шума или эмодзи лучше, чем стандартный BPE/SentencePiece. Преимущество в устойчивости шуму дают byte-level методы (см. ниже).
* **Морфология**: Unigram чувствителен к морфологии: он явно уменьшает разрыв корней и суффиксов. Например, при сравнении удержания корня слова средний охват корня у Unigram выше, чем у BPE. Это напрямую помогает в задачах, где важно распознавать грамматические формы. Однако у Unigram обычно больше токенов на слово (в MorphPiece – на \~17% больше, чем BPE), что слегка удлиняет последовательности, но улучшает моделирование.

**Byte-level BPE (GPT-подобные токенизаторы, tiktoken)**. GPT-2/3/4 (через библиотеку tiktoken) используют BPE не на символах, а на байтах UTF-8. Это позволяет кодировать **любой** символ (в т.ч. эмодзи, нестандартные логограммы) без исключений. Преимущества: модели на byte-BPE могут «понимать» русский, английский, японский, эмодзи и т.д. единым механизмом – нет OOV. Например, эмодзи разбиваются на несколько байтов-токенов, но модель знает их значения. Однако есть и минусы. Во‑первых, UTF-8 даёт разную длину: латиница – 1 байт на букву, кириллица – 2 байта. Таким образом, русский текст кодируется «дороже»: среднее слово разбивается на больше токенов, чем английское того же размера. Это явление называется «byte-premium»: «в BPE на байтах переменная длина кодировки неявно штрафует нелатинские скрипты». Во‑вторых, алгоритм BPE может объединять части *байт*-кодировок символов, создавая токены, не соответствующие целому юникодовому знаку. В работе предлагается даже видеть в токенизаторе GPT-4 «874 таких токена» для частичных UTF-8-последовательностей (пример «<0x95>»). Такие токены не имеют собственного смысла и усложняют обучение модели.

* **Семантическая согласованность**: Byte-BPE абстрагирован от смысловых границ: токены отражают байты, а не морфемы. Он может случайно разрезать лингвистически значимые единицы. Поэтому с точки зрения семантики этот подход далёк от идеала, особенно для сложных языков. Но в английском (1 байт/буква) фрагментация обычно совпадает с буквами или группами букв, так что часто получается адекватно.
* **Следование инструкциям**: GPT-модели (на byte-BPE) хорошо демонстрируют умение следовать инструкциям на английском. Для русского, хотя GPT-модели с byte-BPE корректно обрабатывают кириллицу, богатая морфология может пострадать из-за фрагментации: модель получает больше «кусочков» слова и может хуже обобщать. Прямых исследований нет, но по аналогии с LLaMA+Unigram можно предположить, что смыслового выигрыша у byte-BPE нет.
* **Сегментация и устойчивость**: Здесь byte-BPE выигрывает: любые символы (цифры, эмодзи, иероглифы) разбиваются на байты, а затем на известные подсловные токены. Непредвиденный символ никогда не приведёт к OOV-«<UNK>» – всегда найдётся набор байтов. Это даёт высокую устойчивость к шумам и редким символам.
* **Морфология**: Byte-BPE **не** учитывает морфологию: он сводит текст к потокам байтов. Поэтому ни границы слов, ни основы не соблюдаются. Это самый «морфологически неблагоприятный» метод среди подсловных (корни русских слов могут разбиться пополам по байтам).

**YouTokenToMe (YTTM)**. Этот инструмент («ВашТокенКМеня») от VK реализует тот же BPE, но с оптимизацией скорости. В отличие от многих реализаций (SentencePiece, fastBPE), YTTM тренируется за линейное время по объёму корпуса и использует многопоточность. Например, тесты показали, что при обучении на википедийных корпусах английского, русского и китайского YTTM работает **7–10× быстрее** для «алфавитных» языков (английский, русский) и **40–50× быстрее** для «иероглифических» (китайский). Качество сегментации при этом такое же, как у обычного BPE: YTTM даёт BPE-токены той же природы (за исключением оптимизации). Следовательно, его поведение по критериям семантики/морфологии аналогично описанному для BPE/WordPiece. Основное отличие – существенная экономия времени на обучении токенизатора и токенизации, что важно при очень больших данных. Задачи генерации/перевода при использовании YTTM в остальном дают результаты, сопоставимые с обычным BPE, поскольку этот метод принципиально тот же.

**Резюме по критериям и языкам**:

* **Семантическое соответствие (semantic alignment)**: Токенизаторы, учитывающие лингвистическую структуру (Unigram, MorphPiece) лучше сохраняют смысловые фрагменты слов. BPE и WordPiece часто разрезают слова «вслепую», что ухудшает выравнивание токенов и морфем. Byte-BPE ещё менее лингвистически информативен. Таким образом, для английского и русского языков более «глубокими» считаются подходы с учётом морфологии (Unigram, морфемы), а **BPE/WordPiece** служат компромиссом между размером словаря и представлением.
* **Следование инструкциям (instruction following)**: Практика показывает (на примере адаптации LLM к русскому), что морфологически согласованная токенизация облегчает восприятие инструкций. Модели с Unigram-токенизацией давали более релевантные ответы на русские инструкции по сравнению с оригинальными BPE-моделями. Для английских моделей byte-BPE (GPT) и BPE-методов проблем с инструкциями мало – они стандартизированы под английскую грамматику.
* **Качество сегментации (устойчивость к шуму, пунктуации, эмодзи)**: Byte-level BPE превосходит остальные в покрытии любых символов (включая эмодзи, иностранные письменности) – в нём нет «невидимых» слов. SentencePiece с Unigram/BPE хорошо разделяет пробелы и пунктуацию для языков, где используется алфавит, но не так универсален в мультилингве как byte-BPE. Классические токенизаторы порой не разбивают эмодзи и редко встречающиеся символы, что может приводить к неизвестным токенам. Регулярные выражения часто заточены под конкретный язык (поэтому русские «акценты» или эмодзи могут быть выделены некорректно). С точки зрения шума, byte-BPE наиболее «гибкий», тогда как whitespace-сплит и rule-based чувствительны к любым незнакомым символам.
* **Морфологическое соответствие**: Unigram-токенизация показала лучшую «морфологическую точность» (т.е. совпадение токенов с морфемными границами). Она сохраняет корни и суффиксы целиком. BPE/WordPiece «ломают» морфологию; в русском, например, сливаются окончания и приставки без учёта их роли. Морфологически мотивированные схемы (например, MorphPiece или специализированные морф-токенизаторы) улучшают это, но обычно удлиняют токенизированные последовательности (что может слегка ухудшать эффективность). Классическая пробельная токенизация не учитывает морфемы вовсе (каждое слово – токен), и у морфологически богатых языков даёт огромное разнообразие форм, что плохо для обучения.

В целом, **для английского языка** традиционные BPE/WordPiece-методы дали хорошие результаты (они просты и «подойдут почти всегда»). **Для русского языка** исследования показали явное преимущество морфологически более точных токенизаторов (Unigram/SentencePiece). Например, LLaMA-модель на русском при замене BPE-вокабуляра на Unigram заметно улучшила качество по всем метрикам и быстрее обучалась. Метрики на задачах генерации и понимания (перплексии, SuperGLUE) оказываются выше при морфологически осмысленной токенизации. Между тем GPT-подобные токенизаторы (byte-level BPE) хорошо подходят для мультилингвальных моделей, где нужно «одним махом» учесть и русский, и эмодзи, но они имеют свои ограничения (см. выше).

**Источники:** последние исследования показывают, что алгоритмы, учитывающие морфемы (Unigram, морфологические токенизаторы), обеспечивают лучшее выравнивание смыслов и качество генерации для русского, тогда как классические BPE/WordPiece – более универсальны и широко применяемы, но менее точны с точки зрения морфологии. Также наблюдается, что производительность токенизаторов (по скорости) значительно повышается у новых библиотек (YouTokenToMe), а предобработка (претокенизация) часто рассчитана на английский и может плохо перенестись на другие языки. Каждый метод имеет свои сильные и слабые стороны в разных задачах (генерации, перевода, сегментации), поэтому выбор токенизатора зависит от конкретной языковой задачи и требований к модели.




- [Multilingual Pretokenization](https://arxiv.org/html/2505.24689v1#:~:text=Many%20pretokenizers%20are%20complex%20regular,%E2%80%98all%E2%80%99)
- [MorphBPE](https://arxiv.org/html/2502.00894v1#:~:text=emphasizing%20the%20necessity%20for%20morphology,arab%40strut%20%C2%A0%28rah%2C%20an)
- [Impact of Tokenization on LLaMa Russian Adaptation](https://arxiv.org/html/2312.02598v1#:~:text=Unigram%20and%20BPE%20algorithms%20showed,vocabulary%20substitution%20which%20has%20higher)
- [Impact of Tokenization on LLaMa Russian Adaptation](https://arxiv.org/html/2312.02598v1#:~:text=likelihood,inflected%20languages%20such%20as%20Russian)
- [MorphPiece : A Linguistic Tokenizer for Large Language Models](https://arxiv.org/html/2307.07262v2#:~:text=whitespace%20splitter%20as%20a%20proxy,BPE%20vocabulary%20available%20to%20MorphPiece)
- [MorphPiece : A Linguistic Tokenizer for Large Language Models](https://arxiv.org/html/2307.07262v2#:~:text=MorphGPT%20surpasses%20the%20accuracy%20of,performance%20in%20other%20related%20tasks)
- [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/html/2505.24689v1#:~:text=Beyond%20these%20pretokenization%20difficulties%2C%20the,In%20addition%2C%20the)
- [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/html/2505.24689v1#:~:text=Byte,0x95%3E%5Cn%5Cn)
- [YouTokenToMe: a tool for quick text tokenization from the VK Team | by VK Team | Medium](https://vkteam.medium.com/youtokentome-a-tool-for-quick-text-tokenization-from-the-vk-team-aa6341215c5a#:~:text=The%20graphs%20show%20that%20the,and%20in%20some%20tests%2C%20more)