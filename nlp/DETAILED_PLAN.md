# Детализированный план реализации NLP микросервиса

## ФАЗА 1: ИНФРАСТРУКТУРА И БАЗОВАЯ НАСТРОЙКА

### 1.1.1 Создать базовый Dockerfile

1. Создать файл Dockerfile в папке nlp/
2. Добавить строку FROM python:3.11-slim
3. Добавить установку Poetry через pip
4. Скопировать pyproject.toml и poetry.lock

### 1.1.2 Настроить Poetry зависимости

1. Открыть nlp/pyproject.toml
2. Добавить spacy = "^3.7.0" в dependencies
3. Добавить grpcio = "^1.60.0" в dependencies
4. Добавить neomodel = "^5.2.0" в dependencies

### 1.1.3 Добавить продвинутые ML зависимости

1. Добавить transformers = "^4.35.0" для BioBERT
2. Добавить torch = "^2.1.0" для нейронных сетей
3. Добавить tensorflow = "^2.15.0" для дополнительных моделей
4. Добавить scikit-learn = "^1.3.0" для ML алгоритмов

### 1.1.4 Добавить специализированные NLP библиотеки

1. Добавить nltk = "^3.8" для дополнительной обработки
2. Добавить gensim = "^4.3.0" для семантического моделирования
3. Добавить natasha = "^1.6.0" для русского NLP
4. Добавить pymorphy3 = "^1.2.0" для морфологии

### 1.1.5 Добавить в docker-compose.yml

1. Открыть корневой docker-compose.yml
2. Добавить сервис nlp после существующих сервисов
3. Указать build: ./nlp
4. Добавить depends_on: [neo4j]

### 1.2.1 Создать расширенную proto схему

1. Создать файл nlp/proto/nlp.proto
2. Добавить message TextInput с полями text, text_type, metadata
3. Добавить message LinguisticGraph с узлами и связями
4. Добавить service NLPService с методами для полного анализа

### 1.2.2 Генерировать Python классы

1. Создать скрипт nlp/generate_proto.bat
2. Добавить команду poetry run python -m grpc_tools.protoc
3. Указать параметры для генерации в nlp/generated/
4. Создать папку nlp/generated/

### 1.2.3 Создать gRPC сервер

1. Создать файл nlp/src/grpc_service.py
2. Импортировать сгенерированные классы
3. Создать класс NLPServiceServicer
4. Добавить метод AnalyzeText с заглушкой

### 1.3.1 Настроить подключение к Neo4j

1. Создать файл nlp/src/database.py
2. Импортировать neomodel
3. Добавить функцию init_db() с connection string
4. Добавить обработку ConnectionError

### 1.3.2 Создать расширенные Neomodel модели

1. Создать файл nlp/src/models.py
2. Создать класс TextDocument с метаданными (title, doi, authors, license)
3. Создать класс LinguisticNode для узлов графа
4. Создать класс SemanticRelation для семантических связей

### 1.3.3 Создать модели для лингвистической структуры

1. Создать класс Sentence с позицией в тексте
2. Создать класс Word с POS, lemma, морфологией
3. Создать класс SyntacticRelation для синтаксических связей
4. Создать класс NamedEntity для именованных сущностей

### 1.3.4 Протестировать CRUD операции

1. Создать файл nlp/test_database.py
2. Добавить функцию test_create_document()
3. Добавить функцию test_read_document()
4. Запустить тест и проверить подключение

## ФАЗА 2: БАЗОВАЯ ТЕКСТОВАЯ ОБРАБОТКА

### 2.1.1 Настроить библиотеки для коррекции

1. Добавить pyspellchecker в pyproject.toml
2. Установить зависимости через poetry install
3. Создать файл nlp/src/spell_checker.py
4. Импортировать SpellChecker и настроить для русского

### 2.1.2 Реализовать грамматическую коррекцию

1. Создать функцию fix_grammar(text: str) -> str
2. Использовать базовые правила коррекции
3. Обработать очевидные ошибки (двойные пробелы и т.д.)
4. Вернуть исправленный текст

### 2.1.3 Реализовать пунктуационную коррекцию

1. Создать функцию fix_punctuation(text: str) -> str
2. Добавить правила для точек в конце предложений
3. Исправить пробелы вокруг знаков препинания
4. Протестировать на примере текста

### 2.2.1 Настроить spaCy для русского языка

1. Добавить команду загрузки модели в Dockerfile
2. RUN python -m spacy download ru_core_news_sm
3. Создать файл nlp/src/text_processor.py
4. Импортировать spacy и загрузить русскую модель

### 2.2.2 Определение частей речи

1. Создать функцию get_pos_tags(text: str) -> List[Dict]
2. Обработать текст через spaCy nlp(text)
3. Извлечь pos_ для каждого токена
4. Вернуть список с токенами и их частями речи

### 2.2.3 Выявление членов предложения

1. Создать функцию get_sentence_members(text: str) -> Dict
2. Использовать dep_ атрибут токенов spaCy
3. Найти токены с dep_ == "nsubj" (подлежащее)
4. Найти токены с dep_ == "ROOT" (сказуемое)

### 2.3.1 Named Entity Recognition

1. Создать функцию extract_entities(text: str) -> List[Dict]
2. Использовать doc.ents из spaCy
3. Извлечь text и label_ для каждой сущности
4. Вернуть список именованных сущностей

### 2.3.2 Снятие неоднозначностей

1. Создать функцию disambiguate_words(text: str) -> str
2. Использовать контекстную информацию из spaCy
3. Для каждого многозначного слова выбрать наиболее вероятное значение
4. Записать результат в метаданные

## ФАЗА 3: ПРОДВИНУТЫЕ NLP АЛГОРИТМЫ

### 3.1.1 Настроить BioBERT модель

1. Создать файл nlp/src/biobert_processor.py
2. Импортировать transformers библиотеку
3. Загрузить предобученную BioBERT модель
4. Создать функцию для получения embeddings

### 3.1.2 Настроить многоязычную BERT модель

1. Загрузить bert-base-multilingual-cased модель
2. Создать функцию get_bert_embeddings(text: str)
3. Обработать текст через tokenizer
4. Получить векторные представления

### 3.1.3 Настроить русскую BERT модель

1. Загрузить ruBERT или DeepPavlov BERT
2. Создать специализированную функцию для русского
3. Оптимизировать для научных текстов
4. Протестировать качество embeddings

### 3.2.1 Реализовать анализ перестановок слов

1. Создать файл nlp/src/permutation_analyzer.py
2. Создать функцию generate_word_permutations(sentence: str)
3. Для каждой перестановки получить семантическое значение
4. Сохранить таблицу перестановок в структуру данных

### 3.2.2 Выявить инвариантные к перестановкам слова

1. Создать функцию find_invariant_words(permutation_table)
2. Сравнить семантические векторы всех перестановок
3. Найти слова, не меняющие смысл при перестановке
4. Сохранить список инвариантных слов

### 3.2.3 Реализовать анализ перестановок предложений

1. Создать функцию generate_sentence_permutations(text: str)
2. Получить все возможные порядки предложений
3. Анализировать смысловые изменения
4. Создать таблицу смысловых связей

### 3.3.1 Построить семантические связи

1. Создать файл nlp/src/semantic_analyzer.py
2. Использовать BERT embeddings для выявления связей
3. Создать функцию find_semantic_relations(sentences)
4. Построить граф семантических связей

### 3.3.2 Анализ смыслов в тексте

1. Создать функцию extract_meanings(text: str)
2. Использовать комбинацию spaCy + BERT
3. Выделить ключевые смысловые единицы
4. Связать смыслы с лингвистическими структурами

### 3.3.3 Построение лингвистического графа

1. Создать файл nlp/src/graph_builder.py
2. Создать функцию build_linguistic_graph(analysis_results)
3. Объединить все виды анализа в единый граф
4. Обеспечить возможность восстановления исходного текста

## ФАЗА 4: МЕТРИКИ И ВАЛИДАЦИЯ

### 4.1.1 Реализовать расчет покрытия текста

1. Создать файл nlp/src/coverage_calculator.py
2. Создать функцию calculate_text_coverage(text, graph)
3. Проверить, все ли смыслы попали в граф
4. Вернуть процент покрытия и пропущенные элементы

### 4.1.2 Создать метрики качества анализа

1. Добавить функцию calculate_pos_accuracy()
2. Добавить функцию calculate_ner_precision()
3. Добавить функцию calculate_semantic_coherence()
4. Создать общий индекс качества анализа

### 4.1.3 Валидация графа лингвистической структуры

1. Создать функцию validate_graph_completeness(graph)
2. Проверить все ли связи корректны
3. Проверить возможность восстановления текста
4. Вернуть отчет о проблемах

## ФАЗА 5: ИНТЕГРАЦИЯ И СОХРАНЕНИЕ

### 5.1.1 Сохранение в Neo4j

1. Создать файл nlp/src/neo4j_saver.py
2. Создать функцию save_linguistic_graph(graph)
3. Сохранить все узлы и связи в Neo4j
4. Обеспечить связь с исходным текстом

### 5.1.2 Интеграция с S3 для текстов

1. Добавить boto3 в зависимости
2. Создать функцию upload_processed_text(text, metadata)
3. Сохранить ссылки в Neo4j
4. Обеспечить версионирование обработанных текстов

### 5.1.3 Создание API эндпоинтов

1. Добавить метод ProcessMarkdownText в gRPC
2. Добавить метод GetLinguisticGraph
3. Добавить метод GetTextCoverage
4. Добавить метод ValidateAnalysis

### 5.2.1 Обработчик Markdown текстов

1. Создать файл nlp/src/markdown_processor.py
2. Парсить Markdown структуру
3. Сохранить форматирование в метаданных
4. Обработать текст с учетом структуры

### 5.2.2 Создание результирующего отчета

1. Создать функцию generate_analysis_report()
2. Включить все виды анализа
3. Добавить метрики качества
4. Сформировать JSON с результатами

## ФАЗА 6: ДОПОЛНИТЕЛЬНЫЕ СПЕЦИАЛИЗИРОВАННЫЕ ИНСТРУМЕНТЫ

### 6.1.1 Интеграция с Natasha для русского NLP

1. Создать файл nlp/src/natasha_processor.py
2. Настроить Natasha для извлечения именованных сущностей
3. Дополнить результаты spaCy результатами Natasha
4. Улучшить качество русскоязычного анализа

### 6.1.2 Интеграция с DeepPavlov

1. Добавить deeppavlov в зависимости
2. Настроить специализированные модели для научных текстов
3. Использовать для улучшения NER и анализа смысла
4. Интегрировать в общий pipeline

### 6.1.3 Использование Gensim для семантического моделирования

1. Создать файл nlp/src/gensim_processor.py
2. Обучить Word2Vec на предметной области
3. Создать Doc2Vec модель для документов
4. Использовать для поиска семантических связей

### 6.2.1 Добавить обработку научных текстов

1. Создать специальный парсер для научных статей
2. Выделить цитаты, формулы, таблицы
3. Обработать библиографические ссылки
4. Интегрировать DOI и метаданные авторов

### 6.2.2 Создать классификатор типов текста

1. Обучить модель на типах: научная статья, книга, веб-страница
2. Создать функцию classify_text_type(text)
3. Адаптировать анализ под тип текста
4. Сохранить тип в метаданных

## ДОПОЛНИТЕЛЬНЫЕ КОМПОНЕНТЫ ДЛЯ ПОЛНОТЫ РЕАЛИЗАЦИИ

### 7.1.1 Создать систему кэширования результатов

1. Добавить Redis для кэширования промежуточных результатов
2. Кэшировать BERT embeddings
3. Кэшировать результаты дорогих операций
4. Оптимизировать производительность

### 7.1.2 Добавить мониторинг и логирование

1. Настроить structured logging
2. Добавить метрики производительности
3. Отслеживать качество анализа
4. Создать дашборд для мониторинга

### 7.2.1 Создать систему тестирования

1. Подготовить тестовые тексты с эталонными результатами
2. Создать unit тесты для каждого компонента
3. Добавить интеграционные тесты
4. Настроить автоматический CI/CD

### 7.2.2 Добавить возможность дообучения моделей

1. Создать pipeline для fine-tuning BERT моделей
2. Добавить интерфейс для загрузки обучающих данных
3. Реализовать инкрементальное обучение
4. Валидировать улучшение качества

## Выводы

После полной декомпозиции план включает:

**82 конкретные задачи** организованные в **6 основных фаз + дополнительные компоненты**:

1. **Фаза 1**: Инфраструктура (11 задач) - ~2 дня
2. **Фаза 2**: Базовая обработка (8 задач) - ~2 дня  
3. **Фаза 3**: Продвинутые алгоритмы (9 задач) - ~3 дня
4. **Фаза 4**: Метрики и валидация (3 задачи) - ~1 день
5. **Фаза 5**: Интеграция (5 задач) - ~2 дня
6. **Фаза 6**: Специализированные инструменты (6 задач) - ~2 дня
7. **Дополнительно**: Кэширование, мониторинг, тестирование (4 задачи) - ~1 день

**Итого: ~13 дней работы** для полной реализации всех требований

Теперь план покрывает ВСЕ требования из оригинального плана:
✅ BioBERT и transformer модели  
✅ Таблицы перестановок слов и предложений  
✅ Графовая лингвистическая структура  
✅ Процент покрытия текста алгоритмами  
✅ Сохранение в Neo4j с возможностью восстановления  
✅ Все специализированные библиотеки (natasha, deeppavlov, gensim и т.д.)  
✅ Обработка научных текстов, DOI, авторов, лицензий  
✅ Интеграция с S3 для хранения текстов 