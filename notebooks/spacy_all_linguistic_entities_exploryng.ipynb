{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f46067",
   "metadata": {},
   "source": [
    "# Исследование всех лингвистических сущностей spaCy\n",
    "\n",
    "Этот блокнот демонстрирует полные возможности spaCy для извлечения лингвистических сущностей из английского текста:\n",
    "- Сегментация предложений\n",
    "- Токенизация (слова, пунктуация)\n",
    "- Части речи (POS tagging)\n",
    "- Морфологический анализ\n",
    "- Лемматизация\n",
    "- Синтаксический разбор (dependency parsing)\n",
    "- Распознавание именованных сущностей (NER)\n",
    "- Извлечение именных групп\n",
    "\n",
    "**Итоговый результат**: JSON-файл с полной автоматической разметкой текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dy7ruofiq2u",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Библиотеки импортированы\n",
      "✓ Загружена модель: en_core_web_trf (Transformer - наивысшая точность)\n",
      "✓ spaCy версия: 3.8.7\n",
      "✓ Компоненты пайплайна: ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "print(\"✓ Библиотеки импортированы\")\n",
    "\n",
    "# Попытка загрузить трансформерную модель (самая точная)\n",
    "# Если недоступна, используем большую модель\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "    model_name = \"en_core_web_trf (Transformer - наивысшая точность)\"\n",
    "    print(f\"✓ Загружена модель: {model_name}\")\n",
    "except OSError:\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "        model_name = \"en_core_web_lg (Large - высокая точность)\"\n",
    "        print(f\"✓ Загружена модель: {model_name}\")\n",
    "    except OSError:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        model_name = \"en_core_web_sm (Small - базовая точность)\"\n",
    "        print(f\"✓ Загружена модель: {model_name}\")\n",
    "\n",
    "print(f\"✓ spaCy версия: {spacy.__version__}\")\n",
    "print(f\"✓ Компоненты пайплайна: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6np6wmim",
   "metadata": {},
   "source": [
    "## 1. Загрузка и парсинг текста\n",
    "\n",
    "Загружаем статью из файла и разделяем на:\n",
    "1. **YAML метаданные** (между `---`)\n",
    "2. **Основной текст** для обработки spaCy\n",
    "3. **References** (список литературы в конце)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6oqpiw6nn29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Документ загружен: d:\\Knowledge_Map\\prompts\\Новый формат статьи. Первый этап.eng.md\n",
      "✓ Метаданных: 11 полей\n",
      "✓ Длина основного текста: 41177 символов\n",
      "✓ Количество references: 120\n",
      "\n",
      "--- Превью метаданных ---\n",
      "DOI: 10.1111/febs.12335\n",
      "Journal: The FEBS\n",
      "Authors: ['Paul M. A. Antony', 'Nico J. Diederich', 'Rejko Krüger']...\n",
      "\n",
      "--- Превью текста (первые 200 символов) ---\n",
      "# The hallmarks of Parkinson's disease\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Since the discovery of dopamine as a neurotransmitter in the 1950s, Parkinson's disease (PD) research has generated a rich and complex body of know...\n"
     ]
    }
   ],
   "source": [
    "def parse_document(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Парсит документ, разделяя на метаданные YAML, основной текст и references.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Извлечение YAML метаданных (между --- в начале)\n",
    "    yaml_pattern = r'^---\\n(.*?)\\n---\\n'\n",
    "    yaml_match = re.match(yaml_pattern, content, re.DOTALL)\n",
    "    \n",
    "    if yaml_match:\n",
    "        yaml_content = yaml_match.group(1)\n",
    "        metadata = yaml.safe_load(yaml_content)\n",
    "        # Удаляем YAML из основного текста\n",
    "        remaining_content = content[yaml_match.end():]\n",
    "    else:\n",
    "        metadata = {}\n",
    "        remaining_content = content\n",
    "    \n",
    "    # Извлечение References (секция ## References до конца)\n",
    "    references_pattern = r'\\n## References\\n+(.*?)$'\n",
    "    references_match = re.search(references_pattern, remaining_content, re.DOTALL)\n",
    "    \n",
    "    if references_match:\n",
    "        references_text = references_match.group(1)\n",
    "        main_text = remaining_content[:references_match.start()]\n",
    "        \n",
    "        # Парсинг references: [номер]: текст ссылки\n",
    "        references = {}\n",
    "        ref_lines = references_text.strip().split('\\n')\n",
    "        for line in ref_lines:\n",
    "            ref_match = re.match(r'\\[(\\d+)\\]:\\s*(.+)$', line)\n",
    "            if ref_match:\n",
    "                ref_num = ref_match.group(1)\n",
    "                ref_text = ref_match.group(2).strip()\n",
    "                references[ref_num] = ref_text\n",
    "    else:\n",
    "        references = {}\n",
    "        main_text = remaining_content\n",
    "    \n",
    "    return {\n",
    "        'metadata': metadata,\n",
    "        'main_text': main_text.strip(),\n",
    "        'references': references\n",
    "    }\n",
    "\n",
    "# Загрузка и парсинг документа\n",
    "file_path = r'd:\\Knowledge_Map\\prompts\\Новый формат статьи. Первый этап.eng.md'\n",
    "parsed_doc = parse_document(file_path)\n",
    "\n",
    "print(f\"✓ Документ загружен: {file_path}\")\n",
    "print(f\"✓ Метаданных: {len(parsed_doc['metadata'])} полей\")\n",
    "print(f\"✓ Длина основного текста: {len(parsed_doc['main_text'])} символов\")\n",
    "print(f\"✓ Количество references: {len(parsed_doc['references'])}\")\n",
    "print(f\"\\n--- Превью метаданных ---\")\n",
    "print(f\"DOI: {parsed_doc['metadata'].get('doi', 'N/A')}\")\n",
    "print(f\"Journal: {parsed_doc['metadata'].get('journal', 'N/A')}\")\n",
    "print(f\"Authors: {list(parsed_doc['metadata'].get('authors', {}).keys())[:3]}...\")\n",
    "print(f\"\\n--- Превью текста (первые 200 символов) ---\")\n",
    "print(parsed_doc['main_text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdpb9ogv15r",
   "metadata": {},
   "source": [
    "## 2. Функции извлечения лингвистических признаков\n",
    "\n",
    "Создаём функции для извлечения всех доступных признаков из spaCy:\n",
    "- **Токены**: 20+ атрибутов (текст, лемма, POS, морфология, синтаксис, семантика)\n",
    "- **Предложения**: токены, зависимости, именные группы\n",
    "- **Документ**: полная иерархическая структура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "q5leod4ldde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Функция extract_token_features() создана\n",
      "  Извлекает: текст, лемма, POS, морфология, синтаксис, сущности, флаги, позиция\n"
     ]
    }
   ],
   "source": [
    "def extract_token_features(token: Token) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Извлекает ВСЕ доступные лингвистические признаки из токена spaCy.\n",
    "    \n",
    "    Включает:\n",
    "    - Базовые атрибуты (текст, лемма, форма)\n",
    "    - Части речи (универсальные и детальные)\n",
    "    - Морфологические признаки (время, вид, залог, число, род, падеж и др.)\n",
    "    - Синтаксические связи (главное слово, зависимые)\n",
    "    - Именованные сущности\n",
    "    - Булевы флаги (is_alpha, is_punct, is_stop и др.)\n",
    "    - Позиционная информация\n",
    "    \"\"\"\n",
    "    \n",
    "    # Извлечение морфологических признаков\n",
    "    morphology = {}\n",
    "    if token.morph:\n",
    "        for key in [\"PronType\", \"Number\", \"Case\", \"Gender\", \"Tense\", \n",
    "                    \"Aspect\", \"Mood\", \"Voice\", \"Person\", \"Degree\", \n",
    "                    \"VerbForm\", \"Poss\", \"Reflex\", \"NumType\", \"Foreign\"]:\n",
    "            values = token.morph.get(key)\n",
    "            if values:\n",
    "                morphology[key] = values[0] if len(values) == 1 else list(values)\n",
    "    \n",
    "    # Извлечение информации о сущностях\n",
    "    entity_info = None\n",
    "    if token.ent_type_:\n",
    "        entity_info = {\n",
    "            \"type\": token.ent_type_,\n",
    "            \"iob\": token.ent_iob_\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        # Базовые текстовые атрибуты\n",
    "        \"id\": token.i,\n",
    "        \"text\": token.text,\n",
    "        \"text_with_ws\": token.text_with_ws,\n",
    "        \"lemma\": token.lemma_,\n",
    "        \"lower\": token.lower_,\n",
    "        \"shape\": token.shape_,\n",
    "        \n",
    "        # Части речи\n",
    "        \"pos\": token.pos_,           # Универсальный POS тег\n",
    "        \"tag\": token.tag_,           # Детальный POS тег\n",
    "        \n",
    "        # Синтаксис\n",
    "        \"dep\": token.dep_,           # Синтаксическая роль (зависимость)\n",
    "        \"head_id\": token.head.i,     # ID главного слова\n",
    "        \"head_text\": token.head.text,\n",
    "        \"children_ids\": [child.i for child in token.children],\n",
    "        \"children_texts\": [child.text for child in token.children],\n",
    "        \"n_lefts\": token.n_lefts,    # Количество левых зависимых\n",
    "        \"n_rights\": token.n_rights,  # Количество правых зависимых\n",
    "        \n",
    "        # Морфология\n",
    "        \"morphology\": morphology if morphology else None,\n",
    "        \n",
    "        # Именованные сущности\n",
    "        \"entity\": entity_info,\n",
    "        \n",
    "        # Булевы флаги\n",
    "        \"flags\": {\n",
    "            \"is_alpha\": token.is_alpha,\n",
    "            \"is_ascii\": token.is_ascii,\n",
    "            \"is_digit\": token.is_digit,\n",
    "            \"is_lower\": token.is_lower,\n",
    "            \"is_upper\": token.is_upper,\n",
    "            \"is_title\": token.is_title,\n",
    "            \"is_punct\": token.is_punct,\n",
    "            \"is_space\": token.is_space,\n",
    "            \"is_stop\": token.is_stop,\n",
    "            \"like_num\": token.like_num,\n",
    "            \"like_url\": token.like_url,\n",
    "            \"like_email\": token.like_email,\n",
    "        },\n",
    "        \n",
    "        # Позиционная информация\n",
    "        \"position\": {\n",
    "            \"token_index\": token.i,\n",
    "            \"char_start\": token.idx,\n",
    "            \"char_end\": token.idx + len(token.text)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"✓ Функция extract_token_features() создана\")\n",
    "print(\"  Извлекает: текст, лемма, POS, морфология, синтаксис, сущности, флаги, позиция\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ukg5gx4l33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Функция extract_sentence_features() создана\n",
      "  Извлекает: токены, зависимости, именные группы\n"
     ]
    }
   ],
   "source": [
    "def extract_sentence_features(sent: Span, sent_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Извлекает все признаки уровня предложения.\n",
    "    \n",
    "    Включает:\n",
    "    - Текст предложения и границы\n",
    "    - Все токены с полными признаками\n",
    "    - Синтаксические зависимости (граф head-dependent)\n",
    "    - Именные группы (noun chunks)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Извлечение всех токенов\n",
    "    tokens = [extract_token_features(token) for token in sent]\n",
    "    \n",
    "    # Извлечение синтаксических зависимостей\n",
    "    dependencies = []\n",
    "    for token in sent:\n",
    "        if token.dep_ != \"ROOT\":  # ROOT не имеет главного слова\n",
    "            dependencies.append({\n",
    "                \"dependent_id\": token.i,\n",
    "                \"dependent_text\": token.text,\n",
    "                \"head_id\": token.head.i,\n",
    "                \"head_text\": token.head.text,\n",
    "                \"relation\": token.dep_\n",
    "            })\n",
    "    \n",
    "    # Извлечение именных групп\n",
    "    noun_chunks = []\n",
    "    for chunk in sent.noun_chunks:\n",
    "        noun_chunks.append({\n",
    "            \"text\": chunk.text,\n",
    "            \"root_text\": chunk.root.text,\n",
    "            \"root_id\": chunk.root.i,\n",
    "            \"start_token\": chunk.start,\n",
    "            \"end_token\": chunk.end,\n",
    "            \"start_char\": chunk.start_char,\n",
    "            \"end_char\": chunk.end_char\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"id\": sent_id,\n",
    "        \"text\": sent.text,\n",
    "        \"start_char\": sent.start_char,\n",
    "        \"end_char\": sent.end_char,\n",
    "        \"start_token\": sent.start,\n",
    "        \"end_token\": sent.end,\n",
    "        \"tokens\": tokens,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"noun_chunks\": noun_chunks,\n",
    "        \"num_tokens\": len(tokens),\n",
    "        \"num_dependencies\": len(dependencies),\n",
    "        \"num_noun_chunks\": len(noun_chunks)\n",
    "    }\n",
    "\n",
    "print(\"✓ Функция extract_sentence_features() создана\")\n",
    "print(\"  Извлекает: токены, зависимости, именные группы\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "g1u4vnez90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Функция process_document_with_spacy() создана\n",
      "  Создаёт полную JSON-структуру с метаданными, references и лингвистической разметкой\n"
     ]
    }
   ],
   "source": [
    "def process_document_with_spacy(text: str, metadata: Dict, references: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Обрабатывает документ с помощью spaCy и создаёт полную JSON-структуру.\n",
    "    \n",
    "    Структура выхода:\n",
    "    - metadata: метаданные из YAML\n",
    "    - references: список литературы\n",
    "    - processing_info: информация об обработке\n",
    "    - document: лингвистическая разметка текста\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Обработка текста с помощью spaCy...\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Извлечение предложений\n",
    "    print(\"Извлечение предложений и токенов...\")\n",
    "    sentences = [extract_sentence_features(sent, i) for i, sent in enumerate(doc.sents)]\n",
    "    \n",
    "    # Извлечение всех именованных сущностей\n",
    "    print(\"Извлечение именованных сущностей...\")\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            \"text\": ent.text,\n",
    "            \"type\": ent.label_,\n",
    "            \"start_char\": ent.start_char,\n",
    "            \"end_char\": ent.end_char,\n",
    "            \"start_token\": ent.start,\n",
    "            \"end_token\": ent.end\n",
    "        })\n",
    "    \n",
    "    # Извлечение всех именных групп на уровне документа\n",
    "    print(\"Извлечение именных групп...\")\n",
    "    all_noun_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Найти предложение, которому принадлежит chunk\n",
    "        sent_id = None\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            if chunk.start >= sent.start and chunk.end <= sent.end:\n",
    "                sent_id = i\n",
    "                break\n",
    "        \n",
    "        all_noun_chunks.append({\n",
    "            \"text\": chunk.text,\n",
    "            \"root_text\": chunk.root.text,\n",
    "            \"root_id\": chunk.root.i,\n",
    "            \"sentence_id\": sent_id,\n",
    "            \"start_token\": chunk.start,\n",
    "            \"end_token\": chunk.end,\n",
    "            \"start_char\": chunk.start_char,\n",
    "            \"end_char\": chunk.end_char\n",
    "        })\n",
    "    \n",
    "    # Статистика частей речи\n",
    "    print(\"Подсчёт статистики...\")\n",
    "    pos_counts = {}\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "    \n",
    "    # Статистика типов сущностей\n",
    "    entity_type_counts = {}\n",
    "    for ent in entities:\n",
    "        ent_type = ent[\"type\"]\n",
    "        entity_type_counts[ent_type] = entity_type_counts.get(ent_type, 0) + 1\n",
    "    \n",
    "    # Формирование итоговой структуры\n",
    "    result = {\n",
    "        \"metadata\": metadata,\n",
    "        \"references\": references,\n",
    "        \"processing_info\": {\n",
    "            \"model\": nlp.meta[\"name\"],\n",
    "            \"model_version\": nlp.meta[\"version\"],\n",
    "            \"spacy_version\": spacy.__version__,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"statistics\": {\n",
    "                \"text_length\": len(text),\n",
    "                \"num_sentences\": len(sentences),\n",
    "                \"num_tokens\": len(doc),\n",
    "                \"num_entities\": len(entities),\n",
    "                \"num_noun_chunks\": len(all_noun_chunks),\n",
    "                \"pos_distribution\": pos_counts,\n",
    "                \"entity_type_distribution\": entity_type_counts\n",
    "            }\n",
    "        },\n",
    "        \"document\": {\n",
    "            \"text\": text,\n",
    "            \"sentences\": sentences,\n",
    "            \"entities\": entities,\n",
    "            \"noun_chunks\": all_noun_chunks\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Обработка завершена!\")\n",
    "    print(f\"  - Предложений: {len(sentences)}\")\n",
    "    print(f\"  - Токенов: {len(doc)}\")\n",
    "    print(f\"  - Сущностей: {len(entities)}\")\n",
    "    print(f\"  - Именных групп: {len(all_noun_chunks)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Функция process_document_with_spacy() создана\")\n",
    "print(\"  Создаёт полную JSON-структуру с метаданными, references и лингвистической разметкой\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yut6xl76rgo",
   "metadata": {},
   "source": [
    "## 3. Обработка документа\n",
    "\n",
    "Запускаем полную обработку текста статьи о болезни Паркинсона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "s1g20xzmfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка текста с помощью spaCy...\n",
      "Извлечение предложений и токенов...\n",
      "Извлечение именованных сущностей...\n",
      "Извлечение именных групп...\n",
      "Подсчёт статистики...\n",
      "✓ Обработка завершена!\n",
      "  - Предложений: 290\n",
      "  - Токенов: 7711\n",
      "  - Сущностей: 92\n",
      "  - Именных групп: 1550\n",
      "\n",
      "============================================================\n",
      "СТАТИСТИКА ОБРАБОТКИ\n",
      "============================================================\n",
      "Длина текста: 41,177 символов\n",
      "Предложений: 290\n",
      "Токенов: 7,711\n",
      "Именованных сущностей: 92\n",
      "Именных групп: 1550\n",
      "\n",
      "Распределение частей речи (топ-10):\n",
      "  NOUN      : 1704\n",
      "  PUNCT     : 1039\n",
      "  ADP       :  794\n",
      "  ADJ       :  753\n",
      "  X         :  749\n",
      "  VERB      :  531\n",
      "  DET       :  495\n",
      "  AUX       :  264\n",
      "  PROPN     :  251\n",
      "  ADV       :  224\n",
      "\n",
      "Типы именованных сущностей:\n",
      "  CARDINAL       :  47\n",
      "  DATE           :  12\n",
      "  PERSON         :  10\n",
      "  ORDINAL        :   9\n",
      "  PERCENT        :   7\n",
      "  MONEY          :   6\n",
      "  PRODUCT        :   1\n"
     ]
    }
   ],
   "source": [
    "# Обработка документа\n",
    "annotated_document = process_document_with_spacy(\n",
    "    text=parsed_doc['main_text'],\n",
    "    metadata=parsed_doc['metadata'],\n",
    "    references=parsed_doc['references']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СТАТИСТИКА ОБРАБОТКИ\")\n",
    "print(\"=\"*60)\n",
    "stats = annotated_document['processing_info']['statistics']\n",
    "print(f\"Длина текста: {stats['text_length']:,} символов\")\n",
    "print(f\"Предложений: {stats['num_sentences']}\")\n",
    "print(f\"Токенов: {stats['num_tokens']:,}\")\n",
    "print(f\"Именованных сущностей: {stats['num_entities']}\")\n",
    "print(f\"Именных групп: {stats['num_noun_chunks']}\")\n",
    "\n",
    "print(f\"\\nРаспределение частей речи (топ-10):\")\n",
    "pos_sorted = sorted(stats['pos_distribution'].items(), key=lambda x: x[1], reverse=True)\n",
    "for pos, count in pos_sorted[:10]:\n",
    "    print(f\"  {pos:10s}: {count:4d}\")\n",
    "\n",
    "print(f\"\\nТипы именованных сущностей:\")\n",
    "for ent_type, count in sorted(stats['entity_type_distribution'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {ent_type:15s}: {count:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xdlvrxnnunm",
   "metadata": {},
   "source": [
    "## 4. Экспорт в JSON\n",
    "\n",
    "Сохраняем полную лингвистическую разметку в JSON-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wsrt8qgwwt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ JSON-файл сохранён: d:\\Knowledge_Map\\notebooks\\spacy_annotations_output.json\n",
      "  Размер файла: 6,380,996 байт\n",
      "\n",
      "============================================================\n",
      "СТРУКТУРА JSON\n",
      "============================================================\n",
      "Верхний уровень ключей: ['metadata', 'references', 'processing_info', 'document']\n",
      "\n",
      "metadata содержит: ['doi', 'journal', 'article_type', 'authors', 'affiliations', 'keywords', 'correspondence', 'article_stages', 'abbreviations', 'lince', 'page_sub_info']\n",
      "references содержит: 120 ссылок\n",
      "processing_info содержит: ['model', 'model_version', 'spacy_version', 'processed_at', 'statistics']\n",
      "document содержит: ['text', 'sentences', 'entities', 'noun_chunks']\n"
     ]
    }
   ],
   "source": [
    "# Сохранение в JSON\n",
    "output_path = r'd:\\Knowledge_Map\\notebooks\\spacy_annotations_output.json'\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(annotated_document, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✓ JSON-файл сохранён: {output_path}\")\n",
    "print(f\"  Размер файла: {len(json.dumps(annotated_document, ensure_ascii=False)):,} байт\")\n",
    "\n",
    "# Проверка структуры JSON\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СТРУКТУРА JSON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Верхний уровень ключей: {list(annotated_document.keys())}\")\n",
    "print(f\"\\nmetadata содержит: {list(annotated_document['metadata'].keys())}\")\n",
    "print(f\"references содержит: {len(annotated_document['references'])} ссылок\")\n",
    "print(f\"processing_info содержит: {list(annotated_document['processing_info'].keys())}\")\n",
    "print(f\"document содержит: {list(annotated_document['document'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wswmmbe5emm",
   "metadata": {},
   "source": [
    "## 5. Исследование результатов\n",
    "\n",
    "Посмотрим на примеры извлечённой лингвистической информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "xlv71mtbhm8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ПРИМЕР 1: ПЕРВОЕ ПРЕДЛОЖЕНИЕ С ПОЛНОЙ РАЗМЕТКОЙ\n",
      "================================================================================\n",
      "\n",
      "Предложение #0:\n",
      "Текст: # The hallmarks of Parkinson's disease\n",
      "\n",
      "## Abstract\n",
      "Границы символов: 0 - 51\n",
      "Количество токенов: 11\n",
      "Количество зависимостей: 10\n",
      "Количество именных групп: 3\n",
      "\n",
      "--- Токены с признаками ---\n",
      "\n",
      "Токен #0: '#'\n",
      "  Лемма: #\n",
      "  POS: PUNCT (детальный: NFP)\n",
      "  Синтаксическая роль: punct\n",
      "  Главное слово: 'hallmarks' (id=2)\n",
      "  Флаги: alpha=False, punct=True, stop=False\n",
      "\n",
      "Токен #1: 'The'\n",
      "  Лемма: the\n",
      "  POS: DET (детальный: DT)\n",
      "  Синтаксическая роль: det\n",
      "  Главное слово: 'hallmarks' (id=2)\n",
      "  Морфология: {'PronType': 'Art'}\n",
      "  Флаги: alpha=True, punct=False, stop=True\n",
      "\n",
      "Токен #2: 'hallmarks'\n",
      "  Лемма: hallmark\n",
      "  POS: NOUN (детальный: NNS)\n",
      "  Синтаксическая роль: ROOT\n",
      "  Главное слово: 'hallmarks' (id=2)\n",
      "  Морфология: {'Number': 'Plur'}\n",
      "  Флаги: alpha=True, punct=False, stop=False\n",
      "\n",
      "Токен #3: 'of'\n",
      "  Лемма: of\n",
      "  POS: ADP (детальный: IN)\n",
      "  Синтаксическая роль: prep\n",
      "  Главное слово: 'hallmarks' (id=2)\n",
      "  Флаги: alpha=True, punct=False, stop=True\n",
      "\n",
      "Токен #4: 'Parkinson'\n",
      "  Лемма: Parkinson\n",
      "  POS: PROPN (детальный: NNP)\n",
      "  Синтаксическая роль: poss\n",
      "  Главное слово: 'disease' (id=6)\n",
      "  Морфология: {'Number': 'Sing'}\n",
      "  Флаги: alpha=True, punct=False, stop=False\n",
      "\n",
      "--- Синтаксические зависимости (первые 5) ---\n",
      "'#' --[punct]--> 'hallmarks'\n",
      "'The' --[det]--> 'hallmarks'\n",
      "'of' --[prep]--> 'hallmarks'\n",
      "'Parkinson' --[poss]--> 'disease'\n",
      "''s' --[case]--> 'Parkinson'\n",
      "\n",
      "--- Именные группы ---\n",
      "  '# The hallmarks' (корень: 'hallmarks')\n",
      "  'Parkinson's disease' (корень: 'disease')\n",
      "  '## Abstract' (корень: 'Abstract')\n"
     ]
    }
   ],
   "source": [
    "# Пример 1: Первое предложение с полной разметкой\n",
    "print(\"=\"*80)\n",
    "print(\"ПРИМЕР 1: ПЕРВОЕ ПРЕДЛОЖЕНИЕ С ПОЛНОЙ РАЗМЕТКОЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "first_sent = annotated_document['document']['sentences'][0]\n",
    "print(f\"\\nПредложение #{first_sent['id']}:\")\n",
    "print(f\"Текст: {first_sent['text']}\")\n",
    "print(f\"Границы символов: {first_sent['start_char']} - {first_sent['end_char']}\")\n",
    "print(f\"Количество токенов: {first_sent['num_tokens']}\")\n",
    "print(f\"Количество зависимостей: {first_sent['num_dependencies']}\")\n",
    "print(f\"Количество именных групп: {first_sent['num_noun_chunks']}\")\n",
    "\n",
    "print(f\"\\n--- Токены с признаками ---\")\n",
    "for token in first_sent['tokens'][:5]:  # Первые 5 токенов\n",
    "    print(f\"\\nТокен #{token['id']}: '{token['text']}'\")\n",
    "    print(f\"  Лемма: {token['lemma']}\")\n",
    "    print(f\"  POS: {token['pos']} (детальный: {token['tag']})\")\n",
    "    print(f\"  Синтаксическая роль: {token['dep']}\")\n",
    "    print(f\"  Главное слово: '{token['head_text']}' (id={token['head_id']})\")\n",
    "    if token['morphology']:\n",
    "        print(f\"  Морфология: {token['morphology']}\")\n",
    "    if token['entity']:\n",
    "        print(f\"  Сущность: {token['entity']['type']} ({token['entity']['iob']})\")\n",
    "    print(f\"  Флаги: alpha={token['flags']['is_alpha']}, punct={token['flags']['is_punct']}, stop={token['flags']['is_stop']}\")\n",
    "\n",
    "print(f\"\\n--- Синтаксические зависимости (первые 5) ---\")\n",
    "for dep in first_sent['dependencies'][:5]:\n",
    "    print(f\"'{dep['dependent_text']}' --[{dep['relation']}]--> '{dep['head_text']}'\")\n",
    "\n",
    "if first_sent['noun_chunks']:\n",
    "    print(f\"\\n--- Именные группы ---\")\n",
    "    for chunk in first_sent['noun_chunks']:\n",
    "        print(f\"  '{chunk['text']}' (корень: '{chunk['root_text']}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8sc0rj6apdv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ПРИМЕР 2: ИМЕНОВАННЫЕ СУЩНОСТИ (первые 20)\n",
      "================================================================================\n",
      "DATE            | the 1950s\n",
      "ORDINAL         | second\n",
      "CARDINAL        | 1\n",
      "PERCENT         | ~ 1%\n",
      "DATE            | older than 60 years\n",
      "DATE            | the age of 80 years\n",
      "PERCENT         | 3%\n",
      "PERCENT         | 5-10%\n",
      "DATE            | the early 1990s\n",
      "DATE            | 15 years\n",
      "ORDINAL         | first\n",
      "CARDINAL        | ~ 28\n",
      "CARDINAL        | only six\n",
      "PERCENT         | approximately 30-70%\n",
      "DATE            | age 80 years\n",
      "CARDINAL        | 18\n",
      "CARDINAL        | 1\n",
      "CARDINAL        | 1\n",
      "CARDINAL        | Four\n",
      "CARDINAL        | 150\n"
     ]
    }
   ],
   "source": [
    "# Пример 2: Именованные сущности\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПРИМЕР 2: ИМЕНОВАННЫЕ СУЩНОСТИ (первые 20)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "entities = annotated_document['document']['entities']\n",
    "for ent in entities[:20]:\n",
    "    print(f\"{ent['type']:15s} | {ent['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "g62t9uqyoj8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ПРИМЕР 3: ПРИМЕРЫ СИНТАКСИЧЕСКИХ ОТНОШЕНИЙ\n",
      "================================================================================\n",
      "acl             : 'influenced' --> 'disease', 'targeting' --> 'endeavors'\n",
      "acomp           : 'symptomatic' --> 'is', 'tempting' --> 'is'\n",
      "advcl           : 'revealing' --> 'generated', 'organizing' --> 'propose'\n",
      "advmod          : 'second' --> 'common', 'most' --> 'common'\n",
      "agent           : 'by' --> 'influenced', 'by' --> 'increased'\n",
      "amod            : 'rich' --> 'body', 'related' --> 'disease'\n",
      "appos           : 'Abstract' --> 'hallmarks', 'IPD' --> 'disease'\n",
      "attr            : 'disease' --> 'be', 'disease' --> 'is'\n",
      "aux             : 'has' --> 'generated', 'to' --> 'be'\n",
      "auxpass         : 'is' --> 'increased', 'is' --> 'characterized'\n",
      "case            : ''s' --> 'Parkinson', ''s' --> 'Parkinson'\n",
      "cc              : 'and' --> 'rich', 'and' --> 'genetic'\n",
      "ccomp           : 'be' --> 'revealing', 'experience' --> 'start'\n",
      "compound        : 'disease' --> 'hallmarks', 'research' --> 'endeavors'\n",
      "conj            : 'complex' --> 'rich', 'environmental' --> 'genetic'\n"
     ]
    }
   ],
   "source": [
    "# Пример 3: Примеры синтаксических отношений\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПРИМЕР 3: ПРИМЕРЫ СИНТАКСИЧЕСКИХ ОТНОШЕНИЙ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Собираем примеры разных типов зависимостей\n",
    "dep_examples = {}\n",
    "for sent in annotated_document['document']['sentences']:\n",
    "    for dep in sent['dependencies']:\n",
    "        rel = dep['relation']\n",
    "        if rel not in dep_examples:\n",
    "            dep_examples[rel] = []\n",
    "        if len(dep_examples[rel]) < 2:  # По 2 примера каждого типа\n",
    "            dep_examples[rel].append(f\"'{dep['dependent_text']}' --> '{dep['head_text']}'\")\n",
    "\n",
    "# Показываем примеры\n",
    "for rel in sorted(dep_examples.keys())[:15]:  # Первые 15 типов\n",
    "    examples = \", \".join(dep_examples[rel])\n",
    "    print(f\"{rel:15s} : {examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "jf1nmsyzw8i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ПРИМЕР 4: МОРФОЛОГИЧЕСКИЙ АНАЛИЗ\n",
      "================================================================================\n",
      "Глаголы с временем и видом:\n",
      "  'generated' (лемма: generate) - Tense:Past, Aspect:Perf, Voice:-\n",
      "  'revealing' (лемма: reveal) - Tense:Pres, Aspect:Prog, Voice:-\n",
      "  'related' (лемма: relate) - Tense:Past, Aspect:Perf, Voice:-\n",
      "  'influenced' (лемма: influence) - Tense:Past, Aspect:Perf, Voice:-\n",
      "  'increased' (лемма: increase) - Tense:Past, Aspect:Perf, Voice:-\n",
      "  'explore' (лемма: explore) - Tense:Pres, Aspect:-, Voice:-\n",
      "  'propose' (лемма: propose) - Tense:Pres, Aspect:-, Voice:-\n",
      "  'based' (лемма: base) - Tense:Past, Aspect:Perf, Voice:-\n",
      "  'organizing' (лемма: organize) - Tense:Pres, Aspect:Prog, Voice:-\n",
      "  'encourage' (лемма: encourage) - Tense:Pres, Aspect:-, Voice:-\n",
      "\n",
      "Существительные с числом:\n",
      "  'hallmarks' - Number:Plur\n",
      "  'disease' - Number:Sing\n",
      "  'Abstract' - Number:Sing\n",
      "  'discovery' - Number:Sing\n",
      "  'dopamine' - Number:Sing\n",
      "  'neurotransmitter' - Number:Sing\n",
      "  '1950s' - Number:Plur\n",
      "  'disease' - Number:Sing\n",
      "  'research' - Number:Sing\n",
      "  'body' - Number:Sing\n"
     ]
    }
   ],
   "source": [
    "# Пример 4: Морфологический анализ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПРИМЕР 4: МОРФОЛОГИЧЕСКИЙ АНАЛИЗ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ищем токены с интересной морфологией\n",
    "print(\"Глаголы с временем и видом:\")\n",
    "verb_count = 0\n",
    "for sent in annotated_document['document']['sentences']:\n",
    "    for token in sent['tokens']:\n",
    "        if token['pos'] == 'VERB' and token['morphology']:\n",
    "            morph = token['morphology']\n",
    "            if 'Tense' in morph or 'Aspect' in morph:\n",
    "                tense = morph.get('Tense', '-')\n",
    "                aspect = morph.get('Aspect', '-')\n",
    "                voice = morph.get('Voice', '-')\n",
    "                print(f\"  '{token['text']}' (лемма: {token['lemma']}) - Tense:{tense}, Aspect:{aspect}, Voice:{voice}\")\n",
    "                verb_count += 1\n",
    "                if verb_count >= 10:\n",
    "                    break\n",
    "    if verb_count >= 10:\n",
    "        break\n",
    "\n",
    "print(\"\\nСуществительные с числом:\")\n",
    "noun_count = 0\n",
    "for sent in annotated_document['document']['sentences']:\n",
    "    for token in sent['tokens']:\n",
    "        if token['pos'] == 'NOUN' and token['morphology']:\n",
    "            morph = token['morphology']\n",
    "            if 'Number' in morph:\n",
    "                number = morph.get('Number', '-')\n",
    "                print(f\"  '{token['text']}' - Number:{number}\")\n",
    "                noun_count += 1\n",
    "                if noun_count >= 10:\n",
    "                    break\n",
    "    if noun_count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lcv83q79z2n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ПРИМЕР 5: ВИЗУАЛИЗАЦИЯ С DISPLACY\n",
      "================================================================================\n",
      "Предложение для визуализации: Parkinson's disease is the second most common neurodegenerative disease.\n",
      "\n",
      "Parkinson            | PROPN    | poss         | --> disease\n",
      "'s                   | PART     | case         | --> Parkinson\n",
      "disease              | NOUN     | nsubj        | --> is\n",
      "is                   | AUX      | ROOT         | --> is\n",
      "the                  | DET      | det          | --> disease\n",
      "second               | ADJ      | amod         | --> common\n",
      "most                 | ADV      | advmod       | --> common\n",
      "common               | ADJ      | amod         | --> disease\n",
      "neurodegenerative    | ADJ      | amod         | --> disease\n",
      "disease              | NOUN     | attr         | --> is\n",
      ".                    | PUNCT    | punct        | --> is\n",
      "\n",
      "✓ Визуализация зависимостей сохранена в: d:\\Knowledge_Map\\notebooks\\spacy_dependency_visualization.html\n",
      "✓ Визуализация сущностей сохранена в: d:\\Knowledge_Map\\notebooks\\spacy_entities_visualization.html\n"
     ]
    }
   ],
   "source": [
    "# Пример 5: Визуализация синтаксического дерева (опционально)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПРИМЕР 5: ВИЗУАЛИЗАЦИЯ С DISPLACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "# Берём короткое предложение для визуализации\n",
    "sample_text = \"Parkinson's disease is the second most common neurodegenerative disease.\"\n",
    "doc = nlp(sample_text)\n",
    "\n",
    "print(f\"Предложение для визуализации: {sample_text}\\n\")\n",
    "\n",
    "# Показываем dependency parse в текстовом виде\n",
    "for token in doc:\n",
    "    print(f\"{token.text:20s} | {token.pos_:8s} | {token.dep_:12s} | --> {token.head.text}\")\n",
    "\n",
    "# Сохранение визуализации в HTML (без использования jupyter)\n",
    "# Используем jupyter=False чтобы избежать ошибки импорта IPython.display\n",
    "try:\n",
    "    html = displacy.render(doc, style=\"dep\", page=True, jupyter=False)\n",
    "    html_path = r'd:\\Knowledge_Map\\notebooks\\spacy_dependency_visualization.html'\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    print(f\"\\n✓ Визуализация зависимостей сохранена в: {html_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Не удалось создать HTML-визуализацию: {e}\")\n",
    "\n",
    "# Визуализация именованных сущностей\n",
    "try:\n",
    "    html_ner = displacy.render(doc, style=\"ent\", page=True, jupyter=False)\n",
    "    html_ner_path = r'd:\\Knowledge_Map\\notebooks\\spacy_entities_visualization.html'\n",
    "    with open(html_ner_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_ner)\n",
    "    print(f\"✓ Визуализация сущностей сохранена в: {html_ner_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Не удалось создать HTML-визуализацию сущностей: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k5tecz5mev",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "В этом блокноте мы исследовали **все возможности spaCy** для извлечения лингвистических сущностей из английского текста:\n",
    "\n",
    "### Извлечённые лингвистические признаки:\n",
    "\n",
    "**На уровне токена:**\n",
    "- Базовые: текст, лемма, нижний регистр, форма (shape)\n",
    "- Части речи: универсальный POS (17 категорий), детальный тег\n",
    "- Морфология: время, вид, залог, наклонение, число, род, падеж, лицо, степень и др.\n",
    "- Синтаксис: синтаксическая роль (50+ типов зависимостей), главное слово, зависимые слова\n",
    "- Семантика: тип именованной сущности, IOB-теги\n",
    "- Булевы флаги: 12 флагов (is_alpha, is_punct, is_stop, like_num и др.)\n",
    "- Позиция: индекс токена, смещение символов\n",
    "\n",
    "**На уровне предложения:**\n",
    "- Границы предложения (символы, токены)\n",
    "- Все токены с полными признаками\n",
    "- Граф синтаксических зависимостей\n",
    "- Именные группы с корнями\n",
    "\n",
    "**На уровне документа:**\n",
    "- Метаданные из YAML\n",
    "- Список литературы (references)\n",
    "- Все предложения\n",
    "- Глобальный список именованных сущностей\n",
    "- Глобальный список именных групп\n",
    "- Статистика: распределение POS, типы сущностей\n",
    "\n",
    "### Результаты:\n",
    "- ✅ JSON-файл с полной автоматической разметкой: `spacy_annotations_output.json`\n",
    "- ✅ HTML-визуализация синтаксического дерева: `spacy_dependency_visualization.html`\n",
    "\n",
    "### Применение:\n",
    "Этот подход можно использовать для:\n",
    "- Лингвистического анализа научных текстов\n",
    "- Извлечения структурированной информации\n",
    "- Построения графов знаний\n",
    "- Обучения моделей машинного обучения\n",
    "- Анализа синтаксических паттернов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4vejdaxx53x",
   "metadata": {},
   "source": [
    "## Справочник лингвистических сущностей spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fa3ca",
   "metadata": {},
   "source": [
    "\n",
    "### Универсальные части речи (Universal POS Tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06358741",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Код</th>\n",
    "<th>English</th>\n",
    "<th>Русский</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>ADJ</td><td>Adjective</td><td>Прилагательное</td></tr>\n",
    "<tr><td>ADP</td><td>Adposition</td><td>Предлог</td></tr>\n",
    "<tr><td>ADV</td><td>Adverb</td><td>Наречие</td></tr>\n",
    "<tr><td>AUX</td><td>Auxiliary</td><td>Вспомогательный глагол</td></tr>\n",
    "<tr><td>CCONJ</td><td>Coordinating Conjunction</td><td>Сочинительный союз</td></tr>\n",
    "<tr><td>DET</td><td>Determiner</td><td>Определитель</td></tr>\n",
    "<tr><td>INTJ</td><td>Interjection</td><td>Междометие</td></tr>\n",
    "<tr><td>NOUN</td><td>Noun</td><td>Существительное</td></tr>\n",
    "<tr><td>NUM</td><td>Numeral</td><td>Числительное</td></tr>\n",
    "<tr><td>PART</td><td>Particle</td><td>Частица</td></tr>\n",
    "<tr><td>PRON</td><td>Pronoun</td><td>Местоимение</td></tr>\n",
    "<tr><td>PROPN</td><td>Proper Noun</td><td>Имя собственное</td></tr>\n",
    "<tr><td>PUNCT</td><td>Punctuation</td><td>Пунктуация</td></tr>\n",
    "<tr><td>SCONJ</td><td>Subordinating Conjunction</td><td>Подчинительный союз</td></tr>\n",
    "<tr><td>SYM</td><td>Symbol</td><td>Символ</td></tr>\n",
    "<tr><td>VERB</td><td>Verb</td><td>Глагол</td></tr>\n",
    "<tr><td>X</td><td>Other</td><td>Прочее</td></tr>\n",
    "<tr><td>SPACE</td><td>Space</td><td>Пробел</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87695b11",
   "metadata": {},
   "source": [
    "### Синтаксические зависимости (Dependency Relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725eace4",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Код</th>\n",
    "<th>English</th>\n",
    "<th>Русский</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>acl</td><td>Clausal Modifier of Noun</td><td>Придаточное определение</td></tr>\n",
    "<tr><td>acomp</td><td>Adjectival Complement</td><td>Адъективное дополнение</td></tr>\n",
    "<tr><td>advcl</td><td>Adverbial Clause Modifier</td><td>Обстоятельственное придаточное</td></tr>\n",
    "<tr><td>advmod</td><td>Adverbial Modifier</td><td>Обстоятельство</td></tr>\n",
    "<tr><td>agent</td><td>Agent</td><td>Агенс (действующее лицо)</td></tr>\n",
    "<tr><td>amod</td><td>Adjectival Modifier</td><td>Определение (прилагательное)</td></tr>\n",
    "<tr><td>appos</td><td>Appositional Modifier</td><td>Приложение</td></tr>\n",
    "<tr><td>attr</td><td>Attribute</td><td>Именная часть сказуемого</td></tr>\n",
    "<tr><td>aux</td><td>Auxiliary</td><td>Вспомогательный глагол</td></tr>\n",
    "<tr><td>auxpass</td><td>Passive Auxiliary</td><td>Вспомогательный глагол страдательного залога</td></tr>\n",
    "<tr><td>case</td><td>Case Marking</td><td>Падежный показатель</td></tr>\n",
    "<tr><td>cc</td><td>Coordinating Conjunction</td><td>Сочинительный союз</td></tr>\n",
    "<tr><td>ccomp</td><td>Clausal Complement</td><td>Придаточное дополнение</td></tr>\n",
    "<tr><td>compound</td><td>Compound</td><td>Сложное слово</td></tr>\n",
    "<tr><td>conj</td><td>Conjunct</td><td>Однородный член</td></tr>\n",
    "<tr><td>csubj</td><td>Clausal Subject</td><td>Придаточное подлежащее</td></tr>\n",
    "<tr><td>csubjpass</td><td>Clausal Passive Subject</td><td>Придаточное подлежащее (страдательный залог)</td></tr>\n",
    "<tr><td>dative</td><td>Dative</td><td>Дательный падеж</td></tr>\n",
    "<tr><td>dep</td><td>Unspecified Dependency</td><td>Неопределённая зависимость</td></tr>\n",
    "<tr><td>det</td><td>Determiner</td><td>Определитель</td></tr>\n",
    "<tr><td>dobj</td><td>Direct Object</td><td>Прямое дополнение</td></tr>\n",
    "<tr><td>expl</td><td>Expletive</td><td>Формальное подлежащее</td></tr>\n",
    "<tr><td>intj</td><td>Interjection</td><td>Междометие</td></tr>\n",
    "<tr><td>mark</td><td>Marker</td><td>Маркер (союз)</td></tr>\n",
    "<tr><td>meta</td><td>Meta Modifier</td><td>Мета-модификатор</td></tr>\n",
    "<tr><td>neg</td><td>Negation Modifier</td><td>Отрицание</td></tr>\n",
    "<tr><td>nmod</td><td>Nominal Modifier</td><td>Именной модификатор</td></tr>\n",
    "<tr><td>npadvmod</td><td>Noun Phrase as Adverbial Modifier</td><td>Именная группа в роли обстоятельства</td></tr>\n",
    "<tr><td>nsubj</td><td>Nominal Subject</td><td>Подлежащее</td></tr>\n",
    "<tr><td>nsubjpass</td><td>Passive Nominal Subject</td><td>Подлежащее (страдательный залог)</td></tr>\n",
    "<tr><td>nummod</td><td>Numeric Modifier</td><td>Числовой модификатор</td></tr>\n",
    "<tr><td>oprd</td><td>Object Predicate</td><td>Объектный предикат</td></tr>\n",
    "<tr><td>parataxis</td><td>Parataxis</td><td>Паратаксис (сочинительная связь)</td></tr>\n",
    "<tr><td>pcomp</td><td>Complement of Preposition</td><td>Дополнение предлога</td></tr>\n",
    "<tr><td>pobj</td><td>Object of Preposition</td><td>Объект предлога</td></tr>\n",
    "<tr><td>poss</td><td>Possession Modifier</td><td>Притяжательный модификатор</td></tr>\n",
    "<tr><td>preconj</td><td>Preconjunct</td><td>Предсоюз</td></tr>\n",
    "<tr><td>predet</td><td>Predeterminer</td><td>Предопределитель</td></tr>\n",
    "<tr><td>prep</td><td>Prepositional Modifier</td><td>Предложное дополнение</td></tr>\n",
    "<tr><td>prt</td><td>Particle</td><td>Частица</td></tr>\n",
    "<tr><td>punct</td><td>Punctuation</td><td>Пунктуация</td></tr>\n",
    "<tr><td>quantmod</td><td>Modifier of Quantifier</td><td>Модификатор квантора</td></tr>\n",
    "<tr><td>relcl</td><td>Relative Clause Modifier</td><td>Относительное придаточное</td></tr>\n",
    "<tr><td>ROOT</td><td>Root</td><td>Корень (главное слово предложения)</td></tr>\n",
    "<tr><td>xcomp</td><td>Open Clausal Complement</td><td>Инфинитивное дополнение</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cf058",
   "metadata": {},
   "source": [
    "### Типы именованных сущностей (Named Entity Types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421cf5c",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Код</th>\n",
    "<th>English</th>\n",
    "<th>Русский</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>PERSON</td><td>People, including fictional</td><td>Люди, включая вымышленных персонажей</td></tr>\n",
    "<tr><td>NORP</td><td>Nationalities, Religious, Political groups</td><td>Национальности, религиозные и политические группы</td></tr>\n",
    "<tr><td>FAC</td><td>Buildings, Airports, Highways, Bridges</td><td>Здания, аэропорты, шоссе, мосты</td></tr>\n",
    "<tr><td>ORG</td><td>Companies, Agencies, Institutions</td><td>Компании, агентства, учреждения</td></tr>\n",
    "<tr><td>GPE</td><td>Countries, Cities, States</td><td>Страны, города, штаты</td></tr>\n",
    "<tr><td>LOC</td><td>Non-GPE Locations, Mountain ranges, Bodies of water</td><td>Горные хребты, водоёмы, не-GPE локации</td></tr>\n",
    "<tr><td>PRODUCT</td><td>Objects, Vehicles, Foods (not services)</td><td>Объекты, транспорт, продукты (не услуги)</td></tr>\n",
    "<tr><td>EVENT</td><td>Named Hurricanes, Battles, Wars, Sports events</td><td>Ураганы, битвы, войны, спортивные события</td></tr>\n",
    "<tr><td>WORK_OF_ART</td><td>Titles of Books, Songs, etc.</td><td>Названия книг, песен и т.д.</td></tr>\n",
    "<tr><td>LAW</td><td>Named Documents made into Laws</td><td>Законы и правовые документы</td></tr>\n",
    "<tr><td>LANGUAGE</td><td>Any Named Language</td><td>Любой именованный язык</td></tr>\n",
    "<tr><td>DATE</td><td>Absolute or Relative Dates or Periods</td><td>Абсолютные или относительные даты/периоды</td></tr>\n",
    "<tr><td>TIME</td><td>Times smaller than a Day</td><td>Время (меньше дня)</td></tr>\n",
    "<tr><td>PERCENT</td><td>Percentage (including \"%\")</td><td>Проценты (включая \"%\")</td></tr>\n",
    "<tr><td>MONEY</td><td>Monetary Values (including unit)</td><td>Денежные суммы (с единицами)</td></tr>\n",
    "<tr><td>QUANTITY</td><td>Measurements (weight, distance, etc.)</td><td>Измерения (вес, расстояние и т.д.)</td></tr>\n",
    "<tr><td>ORDINAL</td><td>First, Second, etc.</td><td>Порядковые числительные (первый, второй и т.д.)</td></tr>\n",
    "<tr><td>CARDINAL</td><td>Numerals that do not fall under another type</td><td>Количественные числительные</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319910dd",
   "metadata": {},
   "source": [
    "### Морфологические признаки (Morphological Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8ec88",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Признак</th>\n",
    "<th>Значения</th>\n",
    "<th>Русский</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td>Tense</td><td>Past, Pres, Fut</td><td>Время: прошедшее, настоящее, будущее</td></tr>\n",
    "<tr><td>Aspect</td><td>Perf, Prog, Imp</td><td>Вид: совершенный, продолженный, несовершенный</td></tr>\n",
    "<tr><td>Mood</td><td>Ind, Imp, Sub</td><td>Наклонение: изъявительное, повелительное, сослагательное</td></tr>\n",
    "<tr><td>Voice</td><td>Act, Pass</td><td>Залог: действительный, страдательный</td></tr>\n",
    "<tr><td>Number</td><td>Sing, Plur</td><td>Число: единственное, множественное</td></tr>\n",
    "<tr><td>Person</td><td>1, 2, 3</td><td>Лицо: первое, второе, третье</td></tr>\n",
    "<tr><td>Gender</td><td>Masc, Fem, Neut</td><td>Род: мужской, женский, средний</td></tr>\n",
    "<tr><td>Case</td><td>Nom, Acc, Gen, Dat</td><td>Падеж: именительный, винительный, родительный, дательный</td></tr>\n",
    "<tr><td>Degree</td><td>Pos, Cmp, Sup</td><td>Степень: положительная, сравнительная, превосходная</td></tr>\n",
    "<tr><td>VerbForm</td><td>Fin, Inf, Part, Ger</td><td>Форма глагола: финитная, инфинитив, причастие, герундий</td></tr>\n",
    "<tr><td>PronType</td><td>Prs, Art, Dem, Rel</td><td>Тип местоимения: личное, артикль, указательное, относительное</td></tr>\n",
    "<tr><td>Poss</td><td>Yes</td><td>Притяжательное</td></tr>\n",
    "<tr><td>Reflex</td><td>Yes</td><td>Возвратное</td></tr>\n",
    "<tr><td>NumType</td><td>Card, Ord</td><td>Тип числительного: количественное, порядковое</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge-map-py3.12 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
