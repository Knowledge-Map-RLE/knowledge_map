{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ паттернов преобразования предложений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пригодится когда я буду делать поиск закономерностей в большом корпусе текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def analyze_sentence_pattern(doc):\n",
    "    \"\"\"Анализирует структуру предложения и извлекает ключевые паттерны\"\"\"\n",
    "    patterns = {\n",
    "        'nouns': [],\n",
    "        'verbs': [],\n",
    "        'prepositions': [],\n",
    "        'dependencies': [],\n",
    "        'noun_phrases': [],\n",
    "        'verb_phrases': []\n",
    "    }\n",
    "    \n",
    "    for token in doc:\n",
    "        # Основные части речи\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            patterns['nouns'].append({\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'dep': token.dep_,\n",
    "                'head': token.head.text,\n",
    "                'children': [child.text for child in token.children]\n",
    "            })\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            patterns['verbs'].append({\n",
    "                'text': token.text,\n",
    "                'lemma': token.lemma_,\n",
    "                'dep': token.dep_,\n",
    "                'tense': token.morph.get('Tense', []),\n",
    "                'voice': token.morph.get('Voice', [])\n",
    "            })\n",
    "        elif token.pos_ == \"ADP\":\n",
    "            patterns['prepositions'].append({\n",
    "                'text': token.text,\n",
    "                'dep': token.dep_,\n",
    "                'object': token.head.text\n",
    "            })\n",
    "        \n",
    "        # Зависимости\n",
    "        patterns['dependencies'].append({\n",
    "            'token': token.text,\n",
    "            'dep': token.dep_,\n",
    "            'head': token.head.text,\n",
    "            'pos': token.pos_\n",
    "        })\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def extract_noun_phrases(doc):\n",
    "    \"\"\"Извлекает именные группы\"\"\"\n",
    "    noun_phrases = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_phrases.append({\n",
    "            'text': chunk.text,\n",
    "            'root': chunk.root.text,\n",
    "            'deps': [(t.text, t.dep_) for t in chunk]\n",
    "        })\n",
    "    return noun_phrases\n",
    "\n",
    "def find_transformation_pattern(source_doc, target_doc):\n",
    "    \"\"\"Находит паттерн преобразования между исходным и целевым предложениями\"\"\"\n",
    "    \n",
    "    source_patterns = analyze_sentence_pattern(source_doc)\n",
    "    target_patterns = analyze_sentence_pattern(target_doc)\n",
    "    \n",
    "    # Анализ ключевых изменений\n",
    "    transformations = {\n",
    "        'noun_changes': [],\n",
    "        'verb_changes': [],\n",
    "        'structure_changes': [],\n",
    "        'voice_changes': []\n",
    "    }\n",
    "    \n",
    "    # 1. Анализ существительных\n",
    "    source_nouns = {n['text'].lower(): n for n in source_patterns['nouns']}\n",
    "    target_nouns = {n['text'].lower(): n for n in target_patterns['nouns']}\n",
    "    \n",
    "    for noun in target_nouns:\n",
    "        if noun in source_nouns:\n",
    "            # Существительное сохранилось, но изменилась роль\n",
    "            transformations['noun_changes'].append({\n",
    "                'noun': noun,\n",
    "                'source_role': source_nouns[noun]['dep'],\n",
    "                'target_role': target_nouns[noun]['dep']\n",
    "            })\n",
    "    \n",
    "    # 2. Анализ глаголов\n",
    "    source_verbs = [v['lemma'] for v in source_patterns['verbs']]\n",
    "    target_verbs = [v['lemma'] for v in target_patterns['verbs']]\n",
    "    \n",
    "    # Поиск связи между существительными и глаголами\n",
    "    for target_verb in target_verbs:\n",
    "        for source_noun in source_patterns['nouns']:\n",
    "            if target_verb in source_noun['lemma'] or source_noun['lemma'] in target_verb:\n",
    "                transformations['verb_changes'].append({\n",
    "                    'source_noun': source_noun['text'],\n",
    "                    'target_verb': target_verb,\n",
    "                    'transformation': f\"NOUN({source_noun['text']}) -> VERB({target_verb})\"\n",
    "                })\n",
    "    \n",
    "    # 3. Анализ структуры предложения\n",
    "    source_structure = [d['dep'] for d in source_patterns['dependencies']]\n",
    "    target_structure = [d['dep'] for d in target_patterns['dependencies']]\n",
    "    \n",
    "    transformations['structure_changes'] = {\n",
    "        'source_structure': source_structure,\n",
    "        'target_structure': target_structure\n",
    "    }\n",
    "    \n",
    "    return transformations\n",
    "\n",
    "def print_pattern_analysis(source_text, target_text):\n",
    "    \"\"\"Выводит подробный анализ паттернов\"\"\"\n",
    "    source_doc = nlp(source_text)\n",
    "    target_doc = nlp(target_text)\n",
    "    \n",
    "    print(\"=== АНАЛИЗ ПАТТЕРНОВ ПРЕОБРАЗОВАНИЯ ===\")\n",
    "    print(f\"Исходное: {source_text}\")\n",
    "    print(f\"Целевое: {target_text}\")\n",
    "    print()\n",
    "    \n",
    "    # Структура исходного предложения\n",
    "    print(\"ИСХОДНОЕ ПРЕДЛОЖЕНИЕ:\")\n",
    "    for token in source_doc:\n",
    "        print(f\"  {token.text:<15} {token.pos_:<8} {token.dep_:<10} -> {token.head.text}\")\n",
    "    print()\n",
    "    \n",
    "    # Структура целевого предложения\n",
    "    print(\"ЦЕЛЕВОЕ ПРЕДЛОЖЕНИЕ:\")\n",
    "    for token in target_doc:\n",
    "        print(f\"  {token.text:<15} {token.pos_:<8} {token.dep_:<10} -> {token.head.text}\")\n",
    "    print()\n",
    "    \n",
    "    # Паттерны преобразования\n",
    "    patterns = find_transformation_pattern(source_doc, target_doc)\n",
    "    \n",
    "    print(\"ПАТТЕРНЫ ПРЕОБРАЗОВАНИЯ:\")\n",
    "    for change_type, changes in patterns.items():\n",
    "        if changes:\n",
    "            print(f\"  {change_type}:\")\n",
    "            for change in changes:\n",
    "                print(f\"    {change}\")\n",
    "    print()\n",
    "    \n",
    "    # Специфический анализ для твоего примера\n",
    "    print(\"СПЕЦИФИЧЕСКИЙ АНАЛИЗ:\")\n",
    "    \n",
    "    # Поиск связи \"refutation\" -> \"refuted\"\n",
    "    source_refutation = None\n",
    "    for token in source_doc:\n",
    "        if 'refut' in token.lemma_.lower():\n",
    "            source_refutation = token\n",
    "            break\n",
    "    \n",
    "    target_refuted = None\n",
    "    for token in target_doc:\n",
    "        if 'refut' in token.lemma_.lower():\n",
    "            target_refuted = token\n",
    "            break\n",
    "    \n",
    "    if source_refutation and target_refuted:\n",
    "        print(f\"  NOUN -> VERB: '{source_refutation.text}' -> '{target_refuted.text}'\")\n",
    "        print(f\"  Лемма: {source_refutation.lemma_} -> {target_refuted.lemma_}\")\n",
    "        print(f\"  Морфология: {source_refutation.morph} -> {target_refuted.morph}\")\n",
    "    \n",
    "    # Анализ \"aging mechanisms\"\n",
    "    source_aging = None\n",
    "    for token in source_doc:\n",
    "        if token.text.lower() == 'aging':\n",
    "            source_aging = token\n",
    "            break\n",
    "    \n",
    "    if source_aging:\n",
    "        print(f\"  'aging mechanisms' сохраняется как подлежащее\")\n",
    "    \n",
    "    # Анализ залога\n",
    "    target_voice = None\n",
    "    for token in target_doc:\n",
    "        if token.pos_ == \"VERB\":\n",
    "            voice = token.morph.get('Voice', [])\n",
    "            if voice:\n",
    "                target_voice = voice[0]\n",
    "                break\n",
    "    \n",
    "    if target_voice:\n",
    "        print(f\"  Залог: {target_voice}\")\n",
    "\n",
    "# Тестирование\n",
    "\n",
    "source_text = \"Methods of Refutation of Aging Mechanisms for the Purposes of Biological Immortality\"\n",
    "target_text = \"aging mechanisms have been refuted\"\n",
    "\n",
    "print_pattern_analysis(source_text, target_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
