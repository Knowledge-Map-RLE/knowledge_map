services:
  # Neo4j — база данных для хранения графовых данных
  neo4j:
    image: neo4j:5.13.0
    container_name: knowledge_map_neo4j
    ports:
      - "7474:7474"   # HTTP для Neo4j Browser
      - "7687:7687"   # Bolt
    environment:
      NEO4J_AUTH: neo4j/password
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      # Увеличенные настройки памяти для больших графов
      NEO4J_server_memory_heap_initial__size: "4G"
      NEO4J_server_memory_heap_max__size: "4G"
      NEO4J_server_memory_pagecache_size: "2G"
      NEO4J_server_bolt_advertised__address: "127.0.0.1:7687"
      NEO4J_db_memory_transaction_total_max: "2G"
      NEO4J_db_transaction_timeout: "300m"
      # Оптимизированные JVM настройки
      NEO4J_server_jvm_additional: "-XX:+ExitOnOutOfMemoryError -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UseStringDeduplication"
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_dbms_memory_transaction_total_max: "2G"
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*,gds.*"
      NEO4J_dbms_security_procedures_allowlist: "apoc.*,gds.*"
      # Дополнительные настройки для стабильности
      NEO4J_server_http_enabled: "true"
      NEO4J_server_http_listen__address: "0.0.0.0:7474"
      NEO4J_server_bolt_listen__address: "0.0.0.0:7687"
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      memlock:
        soft: -1
        hard: -1
    volumes:
      - D:/Data/Data_Knowledge_Map/neo4j_data:/data
      - D:/Data/Data_Knowledge_Map/neo4j_logs:/logs
      - D:/Data/Data_Knowledge_Map/neo4j_import:/var/lib/neo4j/import
      - D:/Data/Data_Knowledge_Map/neo4j_plugins:/plugins
    networks:
      - knowledge_map_network
    deploy:
      resources:
        limits:
          memory: 8G      # Увеличено для больших графов
          cpus: '4.0'     # Увеличено для производительности
        reservations:
          memory: 6G      # Увеличена гарантированная память
          cpus: '2.0'     # Увеличено минимальное CPU
    healthcheck:
      test: cypher-shell --username neo4j --password password 'MATCH () RETURN count(*) as count'
      interval: 30s       # Увеличено для стабильности
      timeout: 20s        # Увеличено
      retries: 5
      start_period: 90s   # Больше времени на запуск
    restart: unless-stopped
  redis:
    image: redis/redis-stack:latest
    container_name: knowledge_map_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    environment:
      # Оптимизация Redis для стабильности
      REDIS_ARGS: "--maxmemory 1gb --maxmemory-policy allkeys-lru --save 900 1 --save 300 10 --save 60 10000"
    networks:
      - knowledge_map_network
    deploy:
      resources:
        limits:
          memory: 1.5G    # Ограничено
          cpus: '1.0'     # Ограничено
        reservations:
          memory: 1G      # Исправлено: меньше лимита
          cpus: '0.5'     # Минимальные CPU
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s       # Увеличено
      timeout: 15s        # Увеличено
      retries: 5          # Больше попыток
      start_period: 20s   # Больше времени на запуск

  auth:
    build:
      context: ./auth
      dockerfile: Dockerfile
    container_name: knowledge_map_auth
    restart: unless-stopped
    ports:
      - "50052:50051"  # gRPC порт для auth сервиса
    environment:
      - NEO4J_URI=bolt://neo4j:password@neo4j:7687
      - REDIS_URL=redis://redis:6379
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50051
      - SECRET_KEY=your-secret-key-change-in-production
      - ALGORITHM=HS256
      - ACCESS_TOKEN_EXPIRE_MINUTES=30
      - PASSWORD_MIN_LENGTH=8
      - RECOVERY_KEYS_COUNT=10
      - RECOVERY_KEY_LENGTH=16
      - LOGIN_ATTEMPTS_LIMIT=5
      - LOGIN_ATTEMPTS_WINDOW=300
    networks:
      - knowledge_map_network
    depends_on:
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - auth_proto:/shared/proto
      - ./auth:/app
      - /app/__pycache__
      - /app/src/__pycache__
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s


  # layout:
  #   build:
  #     context: ./layering
  #     dockerfile: Dockerfile
  #   container_name: knowledge_map_layout
  #   restart: unless-stopped
  #   ports:
  #     - "50053:50051"  # gRPC порт для layout сервиса
  #   environment:
  #     - LOG_LEVEL=INFO
  #   networks:
  #     - knowledge_map_network
  #   depends_on:
  #     neo4j:
  #       condition: service_healthy

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: knowledge_map_api
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
      - LAYOUT_SERVICE_HOST=layout
      - LAYOUT_SERVICE_PORT=50051
      - AUTH_SERVICE_HOST=auth
      - AUTH_SERVICE_PORT=50051
      - AI_MODEL_SERVICE_HOST=ai
      - AI_MODEL_SERVICE_PORT=50054
      - S3_ENDPOINT_URL=http://s3:9000
      - S3_ACCESS_KEY=minio
      - S3_SECRET_KEY=minio123456
      - S3_REGION=us-east-1
      - DEBUG=true
    networks:
      - knowledge_map_network
    depends_on:
      neo4j:
        condition: service_healthy
      auth:
        condition: service_started
      ai:
        condition: service_started
    volumes:
      - auth_proto:/shared/proto
      - ./api:/app
      - /app/__pycache__
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    # Увеличены лимиты файловых дескрипторов
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  ai:
    build:
      context: ./ai
      dockerfile: Dockerfile
    container_name: knowledge_map_ai
    restart: unless-stopped
    ports:
      - "50054:50054"  # gRPC port for AI service
    environment:
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50054
      - MODEL_CACHE_DIR=/app/models
      - DEFAULT_MODEL=Qwen/Qwen2.5-0.5B-Instruct
      - MODEL_DEVICE=auto
      - LOG_LEVEL=INFO
      - DEFAULT_MAX_TOKENS=2048
      - DEFAULT_TEMPERATURE=0.7
      - MAX_CONTEXT_LENGTH=18000
      - CHUNK_OVERLAP=200
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    networks:
      - knowledge_map_network
    volumes:
      - D:/Data/Data_Knowledge_Map/ai_models:/app/models
    deploy:
      resources:
        limits:
          memory: 8G      # Increased for Llama model
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    # Enable GPU support if available
    runtime: nvidia
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; channel = grpc.insecure_channel('localhost:50054'); channel.close()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # More time for model download

  client:
    build:
      context: ./client
      dockerfile: Dockerfile
    container_name: knowledge_map_client
    restart: unless-stopped
    ports:
      - "3000:80"
    networks:
      - knowledge_map_network
    depends_on:
      api:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  s3:
    image: minio/minio:RELEASE.2024-01-01T16-36-33Z
    container_name: knowledge_map_s3
    restart: unless-stopped
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # MinIO Console
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123456
      MINIO_CONSOLE_ADDRESS: ":9001"
    command: server /data --console-address ":9001"
    volumes:
      -  D:/Data_Knowledge_Map/s3_data:/data
    networks:
      - knowledge_map_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10s

  # === РАСПРЕДЕЛЁННАЯ УКЛАДКА ГРАФОВ ===
  
  # Основной воркер-менеджер для укладки графов
  layout_worker_manager:
    build:
      context: ./worker_distributed_layering
      dockerfile: Dockerfile
    container_name: knowledge_map_layout_manager
    restart: unless-stopped
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=password
      - REDIS_URL=redis://redis:6379/3
      - CELERY_BROKER_URL=redis://redis:6379/4
      - CELERY_RESULT_BACKEND=redis://redis:6379/5
      - CHUNK_SIZE=8000  # Увеличено с 5000 до 8000 для 4GB
      - MAX_WORKERS=4    # Увеличено с 2 до 4 для 4GB
      - MEMORY_LIMIT_GB=4.0  # Остаётся 4.0
      - ENABLE_NUMBA_JIT=true
      - ENABLE_CYTHON_OPTIMIZATION=true
      - LOG_LEVEL=INFO
      - PROMETHEUS_PORT=9100
    ports:
      - "9100:9100"
    volumes:
      - ./worker_distributed_layering/cypher_procedures:/app/cypher_procedures:ro
    networks:
      - knowledge_map_network
    depends_on:
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: ["python", "main.py", "standalone"]
    healthcheck:
      test: ["CMD", "python", "main.py", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 4G      # ← Остаётся 4G
          cpus: '4.0'     # ← Увеличено с 2.0 до 4.0
        reservations:
          memory: 3G      # ← Увеличено с 2G до 3G
          cpus: '2.0'     # ← Увеличено с 1.0 до 2.0

  # Celery воркер для обработки графов
  # layout_worker_1:
  #   build:
  #     context: ./worker_distributed_layering
  #     dockerfile: Dockerfile
  #   container_name: knowledge_map_layout_worker_1
  #   restart: unless-stopped
  #   environment:
  #     - NEO4J_URI=bolt://neo4j:7687
  #     - NEO4J_USER=neo4j
  #     - NEO4J_PASSWORD=password
  #     - REDIS_URL=redis://redis:6379/3
  #     - CELERY_BROKER_URL=redis://redis:6379/4
  #     - CELERY_RESULT_BACKEND=redis://redis:6379/5
  #     - CHUNK_SIZE=6000  # Увеличено с 3000 до 6000
  #     - MEMORY_LIMIT_GB=4.0  # Увеличено с 3.0 до 4.0
  #     - ENABLE_NUMBA_JIT=true
  #     - LOG_LEVEL=INFO
  #     - WORKER_ID=1
  #   networks:
  #     - knowledge_map_network
  #   depends_on:
  #     neo4j:
  #       condition: service_healthy
  #     redis:
  #       condition: service_healthy
  #     layout_worker_manager:
  #       condition: service_started
  #   command: ["celery", "-A", "src.tasks", "worker", "--loglevel=info", "--concurrency=2", "--queues=graph_processing"]
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 4G      # ← Увеличено с 3G до 4G
  #         cpus: '2.0'     # ← Увеличено с 1.5 до 2.0
  #       reservations:
  #         memory: 2G      # ← Увеличено с 1G до 2G
  #         cpus: '1.0'     # ← Увеличено с 0.5 до 1.0

  # # Воркер для оптимизации (опциональный - запускается при больших графах)
  # layout_optimization_worker:
  #   build:
  #     context: ./worker_distributed_layering
  #     dockerfile: Dockerfile
  #   container_name: knowledge_map_layout_optimizer
  #   restart: unless-stopped
  #   environment:
  #     - NEO4J_URI=bolt://neo4j:7687
  #     - NEO4J_USER=neo4j
  #     - NEO4J_PASSWORD=password
  #     - REDIS_URL=redis://redis:6379/3
  #     - CELERY_BROKER_URL=redis://redis:6379/4
  #     - CELERY_RESULT_BACKEND=redis://redis:6379/5
  #     - MEMORY_LIMIT_GB=3.0
  #     - ENABLE_NUMBA_JIT=true
  #     - ENABLE_CYTHON_OPTIMIZATION=true
  #     - LOG_LEVEL=INFO
  #     - WORKER_ID=optimization
  #   networks:
  #     - knowledge_map_network
  #   depends_on:
  #     neo4j:
  #       condition: service_healthy
  #     redis:
  #       condition: service_healthy
  #     layout_worker_manager:
  #       condition: service_started
  #   command: ["celery", "-A", "src.tasks", "worker", "--loglevel=info", "--concurrency=1", "--queues=optimization"]
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 3G
  #         cpus: '1.0'
  #       reservations:
  #         memory: 1G
  #         cpus: '0.5'
  #   profiles:
  #     - production  # Запускается только при профиле production

  # # Воркер для сохранения результатов
  # layout_persistence_worker:
  #   build:
  #     context: ./worker_distributed_layering
  #     dockerfile: Dockerfile
  #   container_name: knowledge_map_layout_persistence
  #   restart: unless-stopped
  #   environment:
  #     - NEO4J_URI=bolt://neo4j:7687
  #     - NEO4J_USER=neo4j
  #     - NEO4J_PASSWORD=password
  #     - REDIS_URL=redis://redis:6379/3
  #     - CELERY_BROKER_URL=redis://redis:6379/4
  #     - CELERY_RESULT_BACKEND=redis://redis:6379/5
  #     - MEMORY_LIMIT_GB=1.0
  #     - LOG_LEVEL=INFO
  #     - WORKER_ID=persistence
  #   networks:
  #     - knowledge_map_network
  #   depends_on:
  #     neo4j:
  #       condition: service_healthy
  #     redis:
  #       condition: service_healthy
  #     layout_worker_manager:
  #       condition: service_started
  #   command: ["celery", "-A", "src.tasks", "worker", "--loglevel=info", "--concurrency=1", "--queues=persistence"]
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 1G
  #         cpus: '0.5'
  #       reservations:
  #         memory: 512M
  #         cpus: '0.25'

  # # Flower для мониторинга Celery (опциональный)
  # layout_flower:
  #   build:
  #     context: ./worker_distributed_layering
  #     dockerfile: Dockerfile
  #   container_name: knowledge_map_layout_flower
  #   restart: unless-stopped
  #   environment:
  #     - CELERY_BROKER_URL=redis://redis:6379/4
  #     - CELERY_RESULT_BACKEND=redis://redis:6379/5
  #   ports:
  #     - "5555:5555"
  #   networks:
  #     - knowledge_map_network
  #   depends_on:
  #     redis:
  #       condition: service_healthy
  #   command: ["celery", "-A", "src.tasks", "flower", "--port=5555"]
  #   profiles:
  #     - monitoring  # Запускается только с профилем monitoring

volumes:
  neo4j_data:
  neo4j_logs:
  neo4j_import:
  neo4j_plugins:
  s3_data:
  auth_proto:
  ai_models:

networks:
  knowledge_map_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
