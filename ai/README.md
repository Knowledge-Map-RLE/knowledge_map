# AI Model Service –¥–ª—è –ö–∞—Ä—Ç—ã –ó–Ω–∞–Ω–∏–π

AI Model Service - —ç—Ç–æ gRPC –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Hugging Face –º–æ–¥–µ–ª—è–º–∏. –°–µ—Ä–≤–∏—Å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º AI –º–æ–¥–µ–ª—è–º –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—â—ë –Ω–µ –∏–º–µ—é—Ç rule-based –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- ü§ñ **Extensible Model Registry** - –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Ä–µ–µ—Å—Ç—Ä
- üöÄ **GPU Support** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CUDA, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ, —Å fallback –Ω–∞ CPU
- üì¶ **Automatic Chunking** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º
- üîå **gRPC API** - –≤—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏
- üê≥ **Docker Ready** - –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É —á–µ—Ä–µ–∑ Docker —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π NVIDIA GPU

## –¢–µ–∫—É—â–∏–µ –º–æ–¥–µ–ª–∏

- **Qwen/Qwen2.5-0.5B-Instruct** (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
  - Instruction-tuned –º–æ–¥–µ–ª—å –æ—Ç Alibaba
  - –ö–æ–Ω—Ç–µ–∫—Å—Ç: 32k —Ç–æ–∫–µ–Ω–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 18k –¥–ª—è —á–∞–Ω–∫–æ–≤ –≤–æ –∏–∑–±–µ–∂–∞–Ω–∏–µ OOM)
  - –†–∞–∑–º–µ—Ä: 0.5B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - –±—ã—Å—Ç—Ä–∞—è –∏ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è
  - –ó–∞–¥–∞—á–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

- **meta-llama/Llama-3.2-1B-Instruct**
  - Instruction-tuned –º–æ–¥–µ–ª—å –æ—Ç Meta
  - –ö–æ–Ω—Ç–µ–∫—Å—Ç: 128k —Ç–æ–∫–µ–Ω–æ–≤
  - –†–∞–∑–º–µ—Ä: 1B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
  - –ó–∞–¥–∞—á–∏: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
ai/
‚îú‚îÄ‚îÄ proto/
‚îÇ   ‚îî‚îÄ‚îÄ ai_model.proto          # gRPC –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
‚îÇ   ‚îú‚îÄ‚îÄ grpc_server.py          # gRPC —Å–µ—Ä–≤–µ—Ä
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ instruct_model.py   # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è instruction-tuned –º–æ–¥–µ–ª–µ–π
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_registry.py  # –†–µ–µ—Å—Ç—Ä –º–æ–¥–µ–ª–µ–π
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_service.py   # –°–µ—Ä–≤–∏—Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ chunking.py         # –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è chunking
‚îú‚îÄ‚îÄ Dockerfile                  # Multi-stage build —Å CUDA
‚îú‚îÄ‚îÄ pyproject.toml              # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Poetry
‚îî‚îÄ‚îÄ README.md
```

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫

### –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫

#### –°–ø–æ—Å–æ–± 1: –ß–µ—Ä–µ–∑ start_local_dev.ps1 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```powershell
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
.\start_local_dev.ps1

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ host —Å–µ—Ä–≤–∏—Å—ã (–≤–∫–ª—é—á–∞—è AI)
.\start_local_dev.ps1 -HostOnly

# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ª–æ–≥–∏ AI —Å–µ—Ä–≤–∏—Å–∞
.\start_local_dev.ps1 -Logs -Service ai

# –°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
.\start_local_dev.ps1 -Status
```

#### –°–ø–æ—Å–æ–± 2: –ü—Ä—è–º–æ–π –∑–∞–ø—É—Å–∫

```powershell
cd ai

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
poetry install

# –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å proto —Ñ–∞–π–ª—ã
poetry run python -m grpc_tools.protoc -I./proto --python_out=./src --grpc_python_out=./src ./proto/ai_model.proto

# –ó–∞–ø—É—Å—Ç–∏—Ç—å gRPC —Å–µ—Ä–≤–µ—Ä
poetry run python src/grpc_server.py
```

–°–µ—Ä–≤–∏—Å –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –Ω–∞ `localhost:50054`

### –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Docker

#### –¢–æ–ª—å–∫–æ AI —Å–µ—Ä–≤–∏—Å

```bash
# Build
docker build -t knowledge-map-ai ./ai

# Run —Å GPU
docker run --gpus all -p 50054:50054 \
  -v D:/Data/Data_Knowledge_Map/ai_models:/app/models \
  -e MODEL_DEVICE=auto \
  knowledge-map-ai

# Run –±–µ–∑ GPU (CPU only)
docker run -p 50054:50054 \
  -v D:/Data/Data_Knowledge_Map/ai_models:/app/models \
  -e MODEL_DEVICE=cpu \
  knowledge-map-ai
```

#### –í—Å—è —Å–∏—Å—Ç–µ–º–∞ —á–µ—Ä–µ–∑ docker-compose

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
docker-compose up -d

# –¢–æ–ª—å–∫–æ AI —Å–µ—Ä–≤–∏—Å
docker-compose up -d ai

# –õ–æ–≥–∏ AI —Å–µ—Ä–≤–∏—Å–∞
docker-compose logs -f ai
```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ß–µ—Ä–µ–∑ API —Å–µ—Ä–≤–∏—Å (REST)

AI —Å–µ—Ä–≤–∏—Å –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ API —Å–µ—Ä–≤–∏—Å –ø–æ –∞–¥—Ä–µ—Å—É:

```
POST http://localhost:8000/api/ai/{model_id}/
```

–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞:

```bash
curl -X POST "http://localhost:8000/api/ai/Qwen/Qwen2.5-0.5B-Instruct/" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain quantum computing in simple terms.",
    "max_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9
  }'
```

–û—Ç–≤–µ—Ç:

```json
{
  "success": true,
  "generated_text": "Quantum computing is...",
  "message": "Text generated successfully",
  "model_used": "Qwen/Qwen2.5-0.5B-Instruct",
  "input_tokens": 128,
  "output_tokens": 256,
  "chunked": false,
  "num_chunks": 0
}
```

### –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ gRPC

```python
import grpc
from utils.generated import ai_model_pb2, ai_model_pb2_grpc

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
channel = grpc.insecure_channel('localhost:50054')
stub = ai_model_pb2_grpc.AIModelServiceStub(channel)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
request = ai_model_pb2.GenerateTextRequest(
    model_id="Qwen/Qwen2.5-0.5B-Instruct",
    prompt="Your prompt here",
    max_tokens=512,
    temperature=0.7
)

response = stub.GenerateText(request)
print(response.generated_text)
```

## –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Markdown

–û–¥–Ω–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á - —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Markdown —Ñ–∞–π–ª–æ–≤ –æ—Ç Dockling:

```python
import requests

# Raw markdown –æ—Ç Dockling
raw_markdown = """
# Scientific Paper Title
This is a poorly formatted document...
"""

# –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
prompt = f"""You are a scientific document formatter. Transform raw Markdown into clean, canonical format.

Requirements:
1. Add YAML frontmatter (title, authors, date, keywords, abstract)
2. Fix heading hierarchy
3. Fix broken paragraphs
4. Convert tables to HTML with <caption>
5. Convert images to HTML <figure> with <figcaption>
6. Format references as numbered [1]

Raw markdown:
{raw_markdown}

Output ONLY formatted Markdown."""

# –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
response = requests.post(
    "http://localhost:8000/api/ai/Qwen/Qwen2.5-0.5B-Instruct/",
    json={
        "prompt": prompt,
        "max_tokens": 4096,
        "temperature": 0.3,
        "enable_chunking": True
    }
)

formatted_md = response.json()["generated_text"]
```

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

| –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é |
|-----------|----------|--------------|
| `GRPC_HOST` | Host –¥–ª—è gRPC —Å–µ—Ä–≤–µ—Ä–∞ | `0.0.0.0` |
| `GRPC_PORT` | –ü–æ—Ä—Ç –¥–ª—è gRPC —Å–µ—Ä–≤–µ—Ä–∞ | `50054` |
| `MODEL_CACHE_DIR` | –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π | `./models` |
| `DEFAULT_MODEL` | –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é | `Qwen/Qwen2.5-0.5B-Instruct` |
| `MODEL_DEVICE` | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (auto/cpu/cuda) | `auto` |
| `LOG_LEVEL` | –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è | `INFO` |
| `DEFAULT_MAX_TOKENS` | Max —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é | `2048` |
| `DEFAULT_TEMPERATURE` | Temperature –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é | `0.7` |
| `MAX_CONTEXT_LENGTH` | –ú–∞–∫—Å. –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ chunking | `18000` |
| `CHUNK_OVERLAP` | Overlap –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ | `200` |

## –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

1. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å –≤ `src/services/model_registry.py`:

```python
self.register_model(
    ModelConfig(
        model_id="new-model/model-name",
        name="Model Name",
        description="Model description",
        max_context_length=128000,
        model_class="instruct_model.InstructModel",  # –∏–ª–∏ –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å
        default_params={
            "max_tokens": 2048,
            "temperature": 0.7,
        }
    )
)
```

2. –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, —Å–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π —Ñ–∞–π–ª –≤ `src/models/`:

```python
# src/models/custom_model.py
class CustomModel:
    def __init__(self, model_id: str, device: str = "auto"):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        pass

    def generate(self, prompt: str, **kwargs) -> dict:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        return {
            "generated_text": "...",
            "input_tokens": 0,
            "output_tokens": 0,
        }
```

3. –û–±–Ω–æ–≤–∏—Ç–µ `model_class` –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ `"custom_model.CustomModel"`

## Health Check

```bash
# gRPC health check
grpcurl -plaintext localhost:50054 ai_model.AIModelService/HealthCheck

# –ß–µ—Ä–µ–∑ API
curl http://localhost:8000/api/ai/health
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### Qwen 2.5 0.5B (–º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- **GPU (CUDA)**: ~100-200 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
- **CPU**: ~20-40 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
- **–ü–∞–º—è—Ç—å**:
  - –ú–æ–¥–µ–ª—å: ~2GB VRAM/RAM
  - –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (18k –∫–æ–Ω—Ç–µ–∫—Å—Ç): ~4-6GB VRAM/RAM
  - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –º–∏–Ω–∏–º—É–º 8GB VRAM (GTX 1070/1080 –∏–ª–∏ –ª—É—á—à–µ)

### Llama 3.2 1B
- **GPU (CUDA)**: ~50-100 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
- **CPU**: ~10-20 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫
- **–ü–∞–º—è—Ç—å**: 4-8GB RAM (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç batch size)

## Troubleshooting

### –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:
1. –ï—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
2. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ (–º–æ–¥–µ–ª–∏ ~5-10GB)
3. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM (–º–∏–Ω–∏–º—É–º 4GB —Å–≤–æ–±–æ–¥–Ω–æ)

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:
1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ GPU: —Å–º–æ—Ç—Ä–∏—Ç–µ –ª–æ–≥–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
2. CPU –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: —É–≤–µ–ª–∏—á—å—Ç–µ `cpus` –≤ docker-compose
3. –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: –±–æ–ª—å—à–∏–µ –ø—Ä–æ–º–ø—Ç—ã –º–µ–¥–ª–µ–Ω–Ω–µ–µ

### Out of Memory

–°–µ—Ä–≤–∏—Å —É–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ GTX 1070 (8GB VRAM):
- `max_memory` –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ 7GB GPU + 8GB RAM
- `low_cpu_mem_usage=True` —Å–Ω–∏–∂–∞–µ—Ç –ø–∏–∫–æ–≤–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM
- `use_cache=True` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç KV-–∫—ç—à –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- `max_length=18000` –∂–µ—Å—Ç–∫–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞

–ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç OOM:
1. –£–º–µ–Ω—å—à–∏—Ç–µ `MAX_CONTEXT_LENGTH` –¥–æ 12000-14000
2. –£–º–µ–Ω—å—à–∏—Ç–µ `ai_max_generation_tokens` –¥–æ 2048
3. –ó–∞–∫—Ä–æ–π—Ç–µ –¥—Ä—É–≥–∏–µ GPU-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU —Ä–µ–∂–∏–º: `MODEL_DEVICE=cpu`

## –õ–æ–≥–∏

```bash
# –õ–æ–∫–∞–ª—å–Ω–æ
.\start_local_dev.ps1 -Logs -Service ai

# Docker
docker-compose logs -f ai

# –§–∞–π–ª –ª–æ–≥–æ–≤ (–ª–æ–∫–∞–ª—å–Ω–æ)
tail -f ai/logs/ai_model.log
tail -f ai/logs/ai_model_error.log
```

## API Documentation

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ API —Å–µ—Ä–≤–∏—Å–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å—É:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

–†–∞–∑–¥–µ–ª AI Models —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ endpoints –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI –º–æ–¥–µ–ª—è–º–∏.

## –õ–∏—Ü–µ–Ω–∑–∏—è

Part of Knowledge Map project
